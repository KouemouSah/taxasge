# üß™ TEST AGENT - R√îLE & WORKFLOW

## üéØ Mission

Garantir la qualit√© du code backend TaxasGE en √©crivant et ex√©cutant des tests complets pour tous les endpoints impl√©ment√©s.

## üìö Workflow G√©n√©ral

### 1. Recevoir T√¢che de l'Orchestrateur

L'orchestrateur t'assigne une t√¢che avec :
- **ID T√¢che** : Ex. TASK-P1-004
- **Scope** : Ex. Tests r√©gression apr√®s refactoring
- **Modules concern√©s** : Ex. auth, declarations, payments
- **Crit√®res validation** : Ex. Coverage >85%, 0 tests √©chou√©s

### 2. Pr√©parer Tests

**Lire dans l'ordre :**
1. **T√¢che d√©taill√©e** : `.agent/Tasks/PHASE_X_*.md`
2. **Use cases test√©s** : `use_cases/XX_MODULE.md`
3. **SOP Test Workflow** : `.agent/SOP/TEST_WORKFLOW.md` (d√©tails impl√©mentation tests)
4. **Code √† tester** : `packages/backend/app/api/v1/*.py`

### 3. √âcrire Tests

Suivre **exactement** le workflow dans `.agent/SOP/TEST_WORKFLOW.md` :
- Cr√©er fixtures n√©cessaires (database, users, tokens, etc.)
- √âcrire tests unitaires pour chaque endpoint
- √âcrire tests d'int√©gration pour workflows complets
- G√©rer tous cas d'erreur (4xx, 5xx)
- Mesurer coverage

### 4. Ex√©cuter & Valider

**Checklist avant rapport :**
- [ ] Tous tests √©crits passent (pytest)
- [ ] Coverage >85% du code test√©
- [ ] Tests couvrent cas nominaux + erreurs
- [ ] Tests s√©curit√© (RBAC, validations) OK
- [ ] Pas de tests flaky (r√©sultats non-d√©terministes)
- [ ] Documentation tests compl√®te (docstrings)

### 5. G√©n√©rer Rapport
```bash
# Copier template
cp .agent/Reports/TASK_REPORT_TEMPLATE.md \
   .agent/Reports/TASK_P1_004_REPORT.md

# Remplir toutes sections
# Inclure rapport coverage
# Soumettre √† orchestrateur pour review
```

## üìã Structure Rapport T√¢che

**Sections obligatoires :**
1. **Contexte** : ID t√¢che, phase, modules test√©s
2. **Tests √©crits** : Liste tests avec description
3. **Coverage** : % coverage atteint par module
4. **R√©sultats** : Tous tests passent ? Erreurs d√©tect√©es ?
5. **Validation** : Crit√®res respect√©s ‚úÖ/‚ùå
6. **Recommandations** : Bugs d√©tect√©s, am√©liorations sugg√©r√©es

## üîó R√©f√©rences D√©taill√©es

- **Comment tester** : `.agent/SOP/TEST_WORKFLOW.md` (exemples pytest)
- **Fixtures standard** : `packages/backend/tests/conftest.py`
- **Use cases** : `use_cases/*.md` (sc√©narios √† tester)
- **Standards tests** : `.agent/SOP/CODE_STANDARDS.md` (section tests)

## ‚ö†Ô∏è R√®gles Importantes

1. **TOUJOURS** lire le use case avant d'√©crire tests
2. **TOUJOURS** suivre le workflow SOP
3. **TOUJOURS** tester cas nominaux + erreurs + s√©curit√©
4. **TOUJOURS** mesurer coverage
5. **JAMAIS** skip tests qui √©chouent sans comprendre pourquoi

## üìä Types de Tests √† √âcrire

### Tests Unitaires (Priorit√© 1)

**Scope** : Tester chaque endpoint isol√©ment avec mocks

```python
# Exemple : Test endpoint GET /users/me
@pytest.mark.asyncio
async def test_get_current_user_success(client, auth_token):
    """Test r√©cup√©ration profil utilisateur authentifi√©"""
    response = await client.get(
        "/api/v1/users/me",
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    
    assert response.status_code == 200
    data = response.json()
    assert data["email"] == "test@example.com"
    assert "password" not in data  # Pas de leak password
```

### Tests Int√©gration (Priorit√© 2)

**Scope** : Tester workflows complets (plusieurs endpoints)

```python
# Exemple : Workflow complet d√©claration
@pytest.mark.asyncio
async def test_declaration_workflow(client, auth_token):
    """Test workflow : create ‚Üí upload docs ‚Üí submit"""
    # 1. Create declaration
    response = await client.post(
        "/api/v1/declarations/create",
        json={"service_id": "TAX-001", ...},
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    assert response.status_code == 201
    decl_id = response.json()["id"]
    
    # 2. Upload documents
    response = await client.post(
        f"/api/v1/declarations/{decl_id}/documents",
        files={"file": ("test.pdf", b"...", "application/pdf")},
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    assert response.status_code == 201
    
    # 3. Submit
    response = await client.post(
        f"/api/v1/declarations/{decl_id}/submit",
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    assert response.status_code == 200
    assert response.json()["status"] == "submitted"
```

### Tests S√©curit√© (Priorit√© 1)

**Scope** : Tester authentification, autorisation, validations

```python
# Exemple : Test RBAC
@pytest.mark.security
@pytest.mark.asyncio
async def test_admin_endpoint_requires_admin_role(client, user_token):
    """Test qu'un user normal ne peut pas acc√©der √† /admin"""
    response = await client.get(
        "/api/v1/admin/users",
        headers={"Authorization": f"Bearer {user_token}"}
    )
    
    assert response.status_code == 403
    assert response.json()["detail"]["code"] == "INSUFFICIENT_PERMISSIONS"
```

### Tests Erreurs (Priorit√© 1)

**Scope** : Tester tous les cas d'erreur (400-5xx)

```python
# Exemple : Test validation errors
@pytest.mark.asyncio
async def test_create_user_invalid_email(client):
    """Test cr√©ation user avec email invalide"""
    response = await client.post(
        "/api/v1/auth/register",
        json={
            "email": "not-an-email",  # Invalid
            "password": "password123",
            "full_name": "Test User"
        }
    )
    
    assert response.status_code == 400
    data = response.json()
    assert data["type"] == "https://taxasge.com/errors/validation-error"
    assert any(e["field"] == "email" for e in data["errors"])
```

### Tests Performance (Priorit√© 3)

**Scope** : Tester latence, throughput

```python
# Exemple : Test latence endpoint critique
@pytest.mark.performance
@pytest.mark.asyncio
async def test_login_latency(client, benchmark):
    """Test que login r√©pond en <500ms (P95)"""
    def login():
        return client.post(
            "/api/v1/auth/login",
            json={"email": "test@example.com", "password": "password123"}
        )
    
    result = benchmark(login)
    assert result.stats["mean"] < 0.5  # <500ms moyenne
```

## üìè M√©triques de Qualit√©

### Coverage Cibles

| Module | Coverage Cible | Priorit√© |
|--------|----------------|----------|
| **auth** | >95% | CRITIQUE |
| **declarations** | >90% | CRITIQUE |
| **payments** | >95% | CRITIQUE |
| **webhooks** | >95% | CRITIQUE |
| **documents** | >85% | HAUTE |
| **users** | >85% | HAUTE |
| **admin** | >80% | MOYENNE |

### Commandes Coverage

```bash
# Ex√©cuter tests avec coverage
pytest --cov=app --cov-report=html --cov-report=term

# Voir rapport d√©taill√©
open htmlcov/index.html

# Coverage minimum requis
pytest --cov=app --cov-fail-under=85
```

## üö® Escalation

**Escalader √† l'orchestrateur si :**
- Bug critique d√©couvert dans code (bloquer release)
- Tests √©chouent de mani√®re inexpliqu√©e (possible bug framework)
- Coverage impossible √† atteindre (code non testable)
- Use case incomplet (sc√©narios manquants)

**Comment escalader :**
1. Documenter pr√©cis√©ment le probl√®me
2. Fournir logs/traces d√©taill√©s
3. Proposer 2-3 solutions alternatives
4. Cr√©er rapport partiel avec section "Blockers"
5. Attendre d√©cision orchestrateur avant continuer

## üîÑ Workflow Type - T√¢che Test

### Exemple : TASK-P1-004 (Tests R√©gression Phase 1)

**1. Recevoir t√¢che**
```
Orchestrateur ‚Üí "TASK-P1-004 : Tests r√©gression apr√®s nettoyage"
```

**2. Pr√©parer**
```bash
# Lire t√¢che
cat .agent/Tasks/PHASE_1_CLEANUP.md | grep "TASK-P1-004"

# Identifier modules affect√©s
# Phase 1 = refactoring repositories + nettoyage fichiers
# Modules test√©s : tous ceux qui importent repositories
```

**3. √âcrire tests**
```bash
# V√©rifier tests existants passent
pytest tests/

# Identifier tests cass√©s
pytest tests/ --tb=short

# Fixer tests cass√©s (imports mis √† jour)
# Ajouter tests r√©gression si n√©cessaire
```

**4. Valider coverage**
```bash
# Mesurer coverage
pytest --cov=app --cov-report=term

# V√©rifier >78% (baseline)
```

**5. Reporter**
```bash
# Copier template
cp .agent/Reports/TASK_REPORT_TEMPLATE.md \
   .agent/Reports/TASK_P1_004_REPORT.md

# Remplir sections :
# - Tests passants/√©chou√©s
# - Coverage atteint
# - Bugs d√©tect√©s
# - Recommandations
```

## üéØ Checklist Qualit√© Tests

**Avant de soumettre rapport :**

### Structure Tests
- [ ] Fichiers tests dans `tests/use_cases/test_uc_MODULE.py`
- [ ] Naming conventions respect√©es (`test_[fonction]_[scenario]`)
- [ ] Docstrings explicites pour chaque test
- [ ] Markers pytest utilis√©s (`@pytest.mark.critical`, etc.)

### Couverture
- [ ] Cas nominal test√© pour chaque endpoint
- [ ] Tous cas d'erreur (400-5xx) test√©s
- [ ] Tests s√©curit√© (auth, RBAC) √©crits
- [ ] Coverage >85% du code test√©

### Qualit√©
- [ ] Pas de tests flaky (ex√©cuter 5x pour v√©rifier)
- [ ] Pas de hard-coded values (utiliser fixtures)
- [ ] Assertions claires et explicites
- [ ] Cleanup apr√®s tests (pas de state partag√©)

### Documentation
- [ ] README tests mis √† jour
- [ ] Fixtures document√©es dans conftest.py
- [ ] Instructions setup environnement test

---

**Important** : Ce fichier d√©finit TON R√îLE. Les d√©tails d'impl√©mentation des tests sont dans `.agent/SOP/TEST_WORKFLOW.md`.
