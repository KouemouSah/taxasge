# üöÄ PHASE 6 : D√âPLOIEMENT & PRODUCTION

**Dur√©e estim√©e** : 1 semaine  
**Objectif** : Backend Production Ready avec CI/CD + Monitoring + Alertes

---

## VUE D'ENSEMBLE

**Jours** :
- Jours 1-2 : CI/CD Pipeline (GitHub Actions)
- Jours 3-4 : Monitoring (Prometheus + Grafana + Alertes)
- Jour 5 : Documentation + Go-Live Checklist

**Livrables** :
- ‚úÖ Pipeline CI/CD automatique
- ‚úÖ 3 environnements (dev, staging, prod)
- ‚úÖ 12 dashboards Grafana
- ‚úÖ Alertes PagerDuty configur√©es
- ‚úÖ Documentation d√©ploiement compl√®te
- ‚úÖ Runbook incidents

---

## JOURS 1-2 : CI/CD PIPELINE

### TASK-P6-001 : GitHub Actions Workflow

**Agent** : Dev  
**Priorit√©** : CRITIQUE  
**Effort** : 1 jour  

#### Contexte
Pipeline automatique : Test ‚Üí Build ‚Üí Deploy (Staging ‚Üí Production).

#### Workflow GitHub Actions
**Fichier** : `.github/workflows/main.yml`

```yaml
name: TaxasGE Backend CI/CD

on:
  push:
    branches: [ main, develop, staging ]
  pull_request:
    branches: [ main ]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: taxasge/backend

jobs:
  # Job 1 : Tests & Quality
  test:
    name: Tests & Code Quality
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: taxasge_test
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run linters
        run: |
          # Black (formatter check)
          black --check app/
          
          # Flake8 (linter)
          flake8 app/ --max-line-length=88 --extend-ignore=E203
          
          # isort (import sorting check)
          isort --check-only app/
          
          # mypy (type checker)
          mypy app/
      
      - name: Run tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/taxasge_test
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET_KEY: test-secret-key
          ENVIRONMENT: test
        run: |
          # Tests avec coverage
          pytest \
            --cov=app \
            --cov-report=xml \
            --cov-report=term \
            --cov-fail-under=85 \
            -v
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          fail_ci_if_error: false
      
      - name: Security scan (Bandit)
        run: |
          pip install bandit
          bandit -r app/ -f json -o bandit-report.json
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            coverage.xml
            bandit-report.json

  # Job 2 : Build Docker Image
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push'
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=semver,pattern={{version}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
          cache-to: type=registry,ref=${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max

  # Job 3 : Deploy to Staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://api-staging.taxasge.com
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Set K8s context
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG_STAGING }}
      
      - name: Deploy to Staging
        run: |
          kubectl set image deployment/backend \
            backend=${{ needs.build.outputs.image-tag }} \
            -n taxasge-staging
          
          kubectl rollout status deployment/backend \
            -n taxasge-staging \
            --timeout=5m
      
      - name: Run smoke tests
        run: |
          curl -f https://api-staging.taxasge.com/health || exit 1
      
      - name: Notify Slack
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "‚úÖ Deployed to Staging: ${{ github.sha }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # Job 4 : Deploy to Production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://api.taxasge.com
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Set K8s context
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG_PRODUCTION }}
      
      - name: Deploy to Production (Blue-Green)
        run: |
          # Deploy green version
          kubectl set image deployment/backend-green \
            backend=${{ needs.build.outputs.image-tag }} \
            -n taxasge-production
          
          kubectl rollout status deployment/backend-green \
            -n taxasge-production \
            --timeout=5m
          
          # Switch traffic (update service selector)
          kubectl patch service backend \
            -n taxasge-production \
            -p '{"spec":{"selector":{"version":"green"}}}'
          
          # Wait 2 minutes monitoring
          sleep 120
          
          # If OK, scale down blue
          kubectl scale deployment/backend-blue \
            -n taxasge-production \
            --replicas=0
      
      - name: Run production smoke tests
        run: |
          curl -f https://api.taxasge.com/health || exit 1
          curl -f https://api.taxasge.com/api/v1/fiscal-services?limit=1 || exit 1
      
      - name: Notify PagerDuty (Success)
        uses: manuelpinto/pagerduty-actions@v1
        with:
          integration-key: ${{ secrets.PAGERDUTY_INTEGRATION_KEY }}
          event-action: resolve
          summary: "‚úÖ Production deployment successful"
      
      - name: Notify Slack
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "üöÄ Deployed to PRODUCTION: ${{ github.sha }}\nURL: https://api.taxasge.com"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
      
      - name: Create GitHub Release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ github.run_number }}
          release_name: Release v${{ github.run_number }}
          body: |
            Deployed to production at ${{ github.sha }}
            
            Changes: ${{ github.event.head_commit.message }}
```

#### Dockerfile Production
**Fichier** : `Dockerfile`

```dockerfile
# Multi-stage build for optimized image

# Stage 1: Builder
FROM python:3.11-slim as builder

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy Python packages from builder
COPY --from=builder /root/.local /root/.local

# Copy application code
COPY app/ ./app/
COPY main.py .
COPY alembic/ ./alembic/
COPY alembic.ini .

# Set PATH
ENV PATH=/root/.local/bin:$PATH

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

#### Crit√®res Validation
- ‚úÖ Pipeline d√©clench√© automatiquement (push main/develop)
- ‚úÖ Tests passent (coverage >85%)
- ‚úÖ Docker image build√©e et push√©e
- ‚úÖ D√©ploiement staging automatique (branch develop)
- ‚úÖ D√©ploiement production avec approbation (branch main)
- ‚úÖ Smoke tests passent apr√®s d√©ploiement

---

### TASK-P6-002 : Environnements (Dev, Staging, Production)

**Agent** : Dev  
**Priorit√©** : CRITIQUE  
**Effort** : 0.5 jour  

#### 3 Environnements

**1. Development (Local)**
- Base de donn√©es : PostgreSQL local
- Redis : Local
- Configuration : `.env.development`
- URL : http://localhost:8000

**2. Staging**
- Base de donn√©es : PostgreSQL Cloud (replica)
- Redis : Cloud
- Configuration : `.env.staging`
- URL : https://api-staging.taxasge.com
- Purpose : Tests int√©gration / QA

**3. Production**
- Base de donn√©es : PostgreSQL Cloud (production)
- Redis : Cloud (clustered)
- Configuration : `.env.production`
- URL : https://api.taxasge.com
- Blue-Green deployment

#### Configuration par Environnement

**Kubernetes ConfigMaps** :
```yaml
# staging-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-config
  namespace: taxasge-staging
data:
  ENVIRONMENT: "staging"
  LOG_LEVEL: "DEBUG"
  DATABASE_POOL_SIZE: "10"
  CORS_ORIGINS: "https://staging.taxasge.com"
  RATE_LIMIT_ENABLED: "true"

---
# production-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-config
  namespace: taxasge-production
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "INFO"
  DATABASE_POOL_SIZE: "20"
  CORS_ORIGINS: "https://taxasge.com"
  RATE_LIMIT_ENABLED: "true"
```

**Kubernetes Secrets** :
```yaml
# staging-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: backend-secrets
  namespace: taxasge-staging
type: Opaque
stringData:
  DATABASE_URL: "postgresql://user:pass@staging-db:5432/taxasge"
  REDIS_URL: "redis://staging-redis:6379/0"
  JWT_SECRET_KEY: "staging-secret-key"
  BANGE_API_KEY: "staging-bange-key"
  BANGE_WEBHOOK_SECRET: "staging-webhook-secret"

---
# production-secrets.yaml (encrypted)
apiVersion: v1
kind: Secret
metadata:
  name: backend-secrets
  namespace: taxasge-production
type: Opaque
data:
  DATABASE_URL: <base64-encoded>
  REDIS_URL: <base64-encoded>
  JWT_SECRET_KEY: <base64-encoded>
  BANGE_API_KEY: <base64-encoded>
  BANGE_WEBHOOK_SECRET: <base64-encoded>
```

#### Crit√®res Validation
- ‚úÖ 3 environnements configur√©s
- ‚úÖ Secrets s√©par√©s par environnement
- ‚úÖ CORS configur√© correctement
- ‚úÖ Logs niveau appropri√© (DEBUG staging, INFO prod)

---

### TASK-P6-003 : Database Migrations (Alembic)

**Agent** : Dev  
**Priorit√©** : HAUTE  
**Effort** : 0.5 jour  

#### Setup Alembic

**Fichier** : `alembic.ini`
```ini
[alembic]
script_location = alembic
sqlalchemy.url = driver://user:pass@localhost/dbname

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**Fichier** : `alembic/env.py`
```python
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os

# Import models
from app.models import Base

# Alembic Config
config = context.config

# Override sqlalchemy.url from environment
config.set_main_option(
    'sqlalchemy.url',
    os.getenv('DATABASE_URL', 'postgresql://localhost/taxasge')
)

# Setup logging
fileConfig(config.config_file_name)

target_metadata = Base.metadata

def run_migrations_online():
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

run_migrations_online()
```

#### Cr√©er Migration
```bash
# Auto-g√©n√©rer migration depuis models
alembic revision --autogenerate -m "Add webhooks table"

# Migration g√©n√©r√©e dans alembic/versions/xxx_add_webhooks_table.py
```

#### Exemple Migration
**Fichier** : `alembic/versions/001_add_webhooks_table.py`
```python
"""Add webhooks table

Revision ID: 001
Revises: 
Create Date: 2025-10-20

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    """Upgrade database schema"""
    op.create_table(
        'webhook_events',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('event_type', sa.String(50), nullable=False),
        sa.Column('transaction_id', sa.String(100), unique=True),
        sa.Column('payload', postgresql.JSONB, nullable=False),
        sa.Column('signature', sa.String(256), nullable=False),
        sa.Column('signature_valid', sa.Boolean, nullable=False),
        sa.Column('processed', sa.Boolean, default=False),
        sa.Column('processing_time_ms', sa.Integer),
        sa.Column('error', sa.Text),
        sa.Column('created_at', sa.DateTime, server_default=sa.text('NOW()'))
    )
    
    op.create_index('idx_webhook_transaction_id', 'webhook_events', ['transaction_id'])
    op.create_index('idx_webhook_created_at', 'webhook_events', ['created_at'])

def downgrade():
    """Rollback database schema"""
    op.drop_index('idx_webhook_created_at')
    op.drop_index('idx_webhook_transaction_id')
    op.drop_table('webhook_events')
```

#### Ex√©cuter Migrations
```bash
# Staging
export DATABASE_URL="postgresql://user:pass@staging-db:5432/taxasge"
alembic upgrade head

# Production
export DATABASE_URL="postgresql://user:pass@prod-db:5432/taxasge"
alembic upgrade head

# Rollback (si probl√®me)
alembic downgrade -1
```

#### Int√©gration CI/CD
```yaml
# Dans .github/workflows/main.yml
- name: Run database migrations (Staging)
  if: github.ref == 'refs/heads/develop'
  env:
    DATABASE_URL: ${{ secrets.DATABASE_URL_STAGING }}
  run: |
    alembic upgrade head

- name: Run database migrations (Production)
  if: github.ref == 'refs/heads/main'
  env:
    DATABASE_URL: ${{ secrets.DATABASE_URL_PRODUCTION }}
  run: |
    alembic upgrade head
```

#### Crit√®res Validation
- ‚úÖ Alembic configur√©
- ‚úÖ Migrations automatis√©es dans CI/CD
- ‚úÖ Rollback possible
- ‚úÖ Migrations test√©es en staging avant prod

---

## JOURS 3-4 : MONITORING & ALERTES

### TASK-P6-004 : Prometheus + Grafana Setup

**Agent** : Dev  
**Priorit√©** : CRITIQUE  
**Effort** : 1 jour  

#### Installation Prometheus

**Fichier** : `k8s/prometheus/config.yaml`
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
      - job_name: 'taxasge-backend'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - taxasge-production
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: backend
          - source_labels: [__meta_kubernetes_pod_ip]
            target_label: __address__
            replacement: $1:8000
```

**D√©ploiement Prometheus** :
```yaml
# k8s/prometheus/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: storage
          mountPath: /prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: storage
        persistentVolumeClaim:
          claimName: prometheus-storage
```

#### Instrumentation Backend

**Fichier** : `app/middleware/prometheus.py`
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import time

# M√©triques
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

active_requests = Gauge(
    'active_requests',
    'Number of active requests',
    ['method', 'endpoint']
)

# Database metrics
database_queries_total = Counter(
    'database_queries_total',
    'Total database queries',
    ['query_type']
)

database_query_duration_seconds = Histogram(
    'database_query_duration_seconds',
    'Database query latency',
    ['query_type']
)

# Business metrics
declarations_created_total = Counter(
    'declarations_created_total',
    'Total declarations created',
    ['status']
)

payments_received_total = Counter(
    'payments_received_total',
    'Total payments received',
    ['provider']
)

revenue_total = Counter(
    'revenue_total_xaf',
    'Total revenue in XAF'
)

class PrometheusMiddleware(BaseHTTPMiddleware):
    """Middleware pour instrumenter requests HTTP"""
    
    async def dispatch(self, request: Request, call_next):
        method = request.method
        endpoint = request.url.path
        
        # Increment active requests
        active_requests.labels(method=method, endpoint=endpoint).inc()
        
        # Start timer
        start_time = time.time()
        
        try:
            # Process request
            response = await call_next(request)
            status = response.status_code
            
        except Exception as e:
            status = 500
            raise
        
        finally:
            # Record metrics
            duration = time.time() - start_time
            
            http_requests_total.labels(
                method=method,
                endpoint=endpoint,
                status=status
            ).inc()
            
            http_request_duration_seconds.labels(
                method=method,
                endpoint=endpoint
            ).observe(duration)
            
            active_requests.labels(method=method, endpoint=endpoint).dec()
        
        return response

# Endpoint m√©triques
@app.get("/metrics")
async def metrics():
    """Expose Prometheus metrics"""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )
```

**Enregistrer middleware** :
```python
# main.py
from app.middleware.prometheus import PrometheusMiddleware

app.add_middleware(PrometheusMiddleware)
```

#### Installation Grafana

**D√©ploiement Grafana** :
```yaml
# k8s/grafana/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-secrets
              key: admin-password
        volumeMounts:
        - name: storage
          mountPath: /var/lib/grafana
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: grafana-storage
```

#### Crit√®res Validation
- ‚úÖ Prometheus scrape backend /metrics
- ‚úÖ Grafana connect√© √† Prometheus
- ‚úÖ M√©triques HTTP instrument√©es
- ‚úÖ M√©triques business instrument√©es

---

### TASK-P6-005 : 12 Dashboards Grafana

**Agent** : Dev  
**Priorit√©** : HAUTE  
**Effort** : 1 jour  

#### 12 Dashboards √† Cr√©er

**Dashboard 1 : API Overview**
```json
{
  "dashboard": {
    "title": "TaxasGE - API Overview",
    "panels": [
      {
        "title": "Requests Per Second",
        "targets": [{
          "expr": "rate(http_requests_total[5m])"
        }]
      },
      {
        "title": "P95 Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
        }]
      },
      {
        "title": "Error Rate (%)",
        "targets": [{
          "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100"
        }]
      },
      {
        "title": "Active Requests",
        "targets": [{
          "expr": "sum(active_requests)"
        }]
      }
    ]
  }
}
```

**Dashboard 2 : Auth Module**
- Login success/failure rate
- Registration rate
- Token refresh rate
- Active sessions

**Dashboard 3 : Declarations Module**
- Declarations cr√©√©es (par statut)
- Temps moyen traitement
- Taux validation/rejection
- Queue agent (pending)

**Dashboard 4 : Payments Module**
- Paiements re√ßus
- Revenus (XAF)
- Payment providers distribution
- Payment failure rate

**Dashboard 5 : Webhooks Module**
- Webhooks re√ßus
- Signature validation rate
- Processing time P95
- Retry count

**Dashboard 6 : Agents Performance**
- D√©clarations trait√©es par agent
- Temps moyen traitement
- Taux validation
- Queue size

**Dashboard 7 : Database Performance**
- Query latency P95
- Connection pool usage
- Slow queries (>1s)
- Deadlocks

**Dashboard 8 : Cache Performance**
- Redis hit/miss rate
- Cache size
- Evictions
- Latency

**Dashboard 9 : Notifications**
- Notifications envoy√©es (par canal)
- Delivery rate
- Failures par provider

**Dashboard 10 : OCR Performance**
- Documents processed
- OCR quality score
- Processing time
- Provider usage (Google Vision vs Tesseract)

**Dashboard 11 : System Resources**
- CPU usage
- Memory usage
- Disk I/O
- Network traffic

**Dashboard 12 : Business Metrics**
- Total revenus (jour/semaine/mois)
- Nouveaux users
- D√©clarations compl√©t√©es
- Top services fiscaux

#### Export/Import Dashboards
```bash
# Export dashboard
curl -H "Authorization: Bearer $GRAFANA_API_KEY" \
  http://grafana:3000/api/dashboards/uid/api-overview \
  > dashboards/api-overview.json

# Import dashboard
curl -X POST \
  -H "Authorization: Bearer $GRAFANA_API_KEY" \
  -H "Content-Type: application/json" \
  -d @dashboards/api-overview.json \
  http://grafana:3000/api/dashboards/db
```

#### Crit√®res Validation
- ‚úÖ 12 dashboards cr√©√©s
- ‚úÖ Toutes m√©triques affich√©es correctement
- ‚úÖ Dashboards export√©s en JSON (versionn√©s)

---

### TASK-P6-006 : Alertes PagerDuty

**Agent** : Dev  
**Priorit√©** : CRITIQUE  
**Effort** : 0.5 jour  

#### Configuration Alertmanager

**Fichier** : `k8s/prometheus/alertmanager-config.yaml`
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
    
    route:
      receiver: 'pagerduty'
      group_by: ['alertname', 'severity']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      
      routes:
        - match:
            severity: critical
          receiver: 'pagerduty-critical'
          continue: true
        
        - match:
            severity: warning
          receiver: 'slack-warnings'
    
    receivers:
      - name: 'pagerduty-critical'
        pagerduty_configs:
          - service_key: '<PAGERDUTY_SERVICE_KEY>'
            description: '{{ .CommonAnnotations.summary }}'
            details:
              firing: '{{ .Alerts.Firing | len }}'
              details: '{{ .CommonAnnotations.description }}'
      
      - name: 'slack-warnings'
        slack_configs:
          - api_url: '<SLACK_WEBHOOK_URL>'
            channel: '#taxasge-alerts'
            title: '{{ .CommonAnnotations.summary }}'
            text: '{{ .CommonAnnotations.description }}'
```

#### R√®gles d'Alertes

**Fichier** : `k8s/prometheus/alert-rules.yaml`
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: monitoring
data:
  alerts.yml: |
    groups:
      - name: api_alerts
        interval: 30s
        rules:
          # CRITICAL : API Down
          - alert: APIDown
            expr: up{job="taxasge-backend"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "API is down"
              description: "Backend API has been down for more than 1 minute"
          
          # CRITICAL : High Error Rate
          - alert: HighErrorRate
            expr: |
              sum(rate(http_requests_total{status=~"5.."}[5m]))
              /
              sum(rate(http_requests_total[5m]))
              > 0.05
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "High error rate (>5%)"
              description: "Error rate is {{ $value }}%"
          
          # CRITICAL : Database Connection Lost
          - alert: DatabaseConnectionLost
            expr: database_connections_active == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Database connection lost"
              description: "No active database connections"
          
          # CRITICAL : Webhook Processing Failures
          - alert: WebhookProcessingFailures
            expr: |
              rate(webhook_errors_total[5m]) > 10
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "High webhook processing failures"
              description: ">10 webhooks/5min failed"
          
          # WARNING : High Latency
          - alert: HighLatency
            expr: |
              histogram_quantile(0.95,
                rate(http_request_duration_seconds_bucket[5m])
              ) > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High P95 latency (>1s)"
              description: "P95 latency is {{ $value }}s"
          
          # WARNING : High CPU Usage
          - alert: HighCPUUsage
            expr: |
              rate(process_cpu_seconds_total[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage (>80%)"
              description: "CPU usage is {{ $value }}%"
          
          # WARNING : High Memory Usage
          - alert: HighMemoryUsage
            expr: |
              process_resident_memory_bytes / 1e9 > 2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage (>2GB)"
              description: "Memory usage is {{ $value }}GB"
          
          # WARNING : Disk Space Low
          - alert: DiskSpaceLow
            expr: |
              node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Disk space <10%"
              description: "Only {{ $value }}% disk space available"
```

#### Test Alertes
```bash
# D√©clencher alerte manuellement
curl -X POST http://prometheus:9090/-/reload

# V√©rifier alertes actives
curl http://prometheus:9090/api/v1/alerts | jq .

# Simuler alerte (kill backend)
kubectl scale deployment/backend --replicas=0 -n taxasge-production
# ‚Üí Alerte APIDown se d√©clenche apr√®s 1min

# Restaurer
kubectl scale deployment/backend --replicas=3 -n taxasge-production
```

#### Crit√®res Validation
- ‚úÖ Alertes configur√©es (7 alertes critiques + warnings)
- ‚úÖ PagerDuty int√©gr√©
- ‚úÖ Slack notifications configur√©es
- ‚úÖ Tests alertes passants

---

## JOUR 5 : DOCUMENTATION & GO-LIVE

### TASK-P6-007 : Documentation D√©ploiement

**Agent** : Doc  
**Priorit√©** : HAUTE  
**Effort** : 0.5 jour  

#### Documentation Compl√®te

**Fichier** : `docs/DEPLOYMENT.md`
```markdown
# TaxasGE Backend - Guide D√©ploiement

## Architecture Infrastructure

### Production
- **Kubernetes** : GKE (Google Kubernetes Engine)
- **Database** : PostgreSQL 15 (Cloud SQL)
- **Cache** : Redis 7 (Cloud Memorystore)
- **Storage** : Firebase Storage
- **CDN** : Cloudflare
- **Monitoring** : Prometheus + Grafana
- **Alerting** : PagerDuty + Slack

### Environnements
1. **Development** : http://localhost:8000
2. **Staging** : https://api-staging.taxasge.com
3. **Production** : https://api.taxasge.com

## Proc√©dure D√©ploiement

### D√©ploiement Automatique (CI/CD)
```bash
# Push vers branch develop ‚Üí Deploy staging
git push origin develop

# Push vers branch main ‚Üí Deploy production
git push origin main
```

### D√©ploiement Manuel
```bash
# 1. Build image
docker build -t ghcr.io/taxasge/backend:v1.0.0 .

# 2. Push image
docker push ghcr.io/taxasge/backend:v1.0.0

# 3. Update deployment
kubectl set image deployment/backend \
  backend=ghcr.io/taxasge/backend:v1.0.0 \
  -n taxasge-production

# 4. Monitor rollout
kubectl rollout status deployment/backend -n taxasge-production
```

## Rollback Proc√©dure

### Rollback Automatique
```bash
# Rollback au deployment pr√©c√©dent
kubectl rollout undo deployment/backend -n taxasge-production

# Rollback √† version sp√©cifique
kubectl rollout undo deployment/backend \
  --to-revision=5 \
  -n taxasge-production
```

### Rollback Database
```bash
# Downgrade migration
alembic downgrade -1
```

## Configuration Secrets

### Ajouter Secret
```bash
kubectl create secret generic backend-secrets \
  --from-literal=DATABASE_URL=postgresql://... \
  --from-literal=JWT_SECRET_KEY=... \
  -n taxasge-production
```

### Mettre √† jour Secret
```bash
kubectl edit secret backend-secrets -n taxasge-production
```

## Monitoring

### Acc√®s Dashboards
- Grafana : https://grafana.taxasge.com
- Prometheus : https://prometheus.taxasge.com
- PagerDuty : https://taxasge.pagerduty.com

### M√©triques Cl√©s
- Requests/sec : >1000
- P95 Latency : <500ms
- Error Rate : <1%
- Uptime : >99.9%
```

#### Crit√®res Validation
- ‚úÖ Documentation compl√®te
- ‚úÖ Proc√©dures claires
- ‚úÖ Rollback document√©

---

### TASK-P6-008 : Runbook Incidents

**Agent** : Doc  
**Priorit√©** : CRITIQUE  
**Effort** : 0.5 jour  

#### Runbook Complet

**Fichier** : `docs/RUNBOOK.md`
```markdown
# RUNBOOK - Incidents Production

## Incident 1 : API Down

**Alerte** : APIDown (PagerDuty CRITICAL)

**Diagnostic** :
```bash
# Check pods status
kubectl get pods -n taxasge-production

# Check logs
kubectl logs -l app=backend -n taxasge-production --tail=100
```

**Actions** :
1. V√©rifier sant√© pods : `kubectl get pods`
2. Si CrashLoopBackOff ‚Üí Check logs erreur
3. Si OOMKilled ‚Üí Augmenter memory limit
4. Rollback si d√©ploiement r√©cent : `kubectl rollout undo`
5. Scale up replicas : `kubectl scale deployment/backend --replicas=5`

**Escalation** : Si non r√©solu en 15min ‚Üí Call Lead Dev

---

## Incident 2 : High Error Rate (>5%)

**Alerte** : HighErrorRate (PagerDuty CRITICAL)

**Diagnostic** :
```bash
# Identifier endpoints en erreur
curl https://api.taxasge.com/metrics | grep http_requests_total

# Check logs erreurs
kubectl logs -l app=backend -n taxasge-production | grep ERROR
```

**Actions** :
1. Identifier endpoint probl√©matique
2. Check database connexion : `SELECT 1`
3. Check Redis connexion : `PING`
4. Check external APIs (BANGE, Firebase)
5. Si database timeout ‚Üí Scale DB replicas
6. Si rate limiting ‚Üí Adjust limits

**Escalation** : Si erreurs persistent >10min ‚Üí Call Team

---

## Incident 3 : Database Connection Lost

**Alerte** : DatabaseConnectionLost (PagerDuty CRITICAL)

**Diagnostic** :
```bash
# Test connexion DB
kubectl exec -it backend-pod -- \
  psql $DATABASE_URL -c "SELECT 1"

# Check Cloud SQL status
gcloud sql instances describe taxasge-prod
```

**Actions** :
1. V√©rifier Cloud SQL up
2. Check connection pool : `SHOW pool_size`
3. Check firewall rules
4. Restart pods : `kubectl rollout restart deployment/backend`
5. Si database down ‚Üí Escalate to GCP support

**Escalation** : IMMEDIATE ‚Üí Call DBA + CTO

---

## Incident 4 : Webhooks BANGE Failing

**Alerte** : WebhookProcessingFailures (PagerDuty CRITICAL)

**Diagnostic** :
```bash
# Check webhook logs
kubectl logs -l app=backend | grep webhook

# Query webhook_events table
psql -c "SELECT * FROM webhook_events WHERE processed=false LIMIT 10"
```

**Actions** :
1. V√©rifier signature BANGE (token expir√© ?)
2. Check BANGE API status : https://status.bange.com
3. V√©rifier connectivit√© BANGE
4. Manual retry webhooks :
   ```python
   python scripts/retry_webhooks.py --failed-only
   ```
5. Contact BANGE support si probl√®me persiste

**Escalation** : Si >100 webhooks failed ‚Üí Call Finance Team

---

## Incident 5 : High Latency (P95 >1s)

**Alerte** : HighLatency (PagerDuty WARNING)

**Diagnostic** :
```bash
# Identifier endpoints lents
curl https://api.taxasge.com/metrics | grep duration_seconds

# Check slow queries
psql -c "SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10"
```

**Actions** :
1. Identifier endpoint lent
2. Check DB slow queries
3. Add missing indexes :
   ```sql
   CREATE INDEX idx_declarations_user_id ON declarations(user_id);
   ```
4. Scale up pods : `kubectl scale deployment/backend --replicas=6`
5. Enable Redis caching

**Escalation** : Si latence persist >30min ‚Üí Call Tech Lead

---

## Incident 6 : Disk Space Low (<10%)

**Alerte** : DiskSpaceLow (PagerDuty WARNING)

**Diagnostic** :
```bash
# Check disk usage
df -h

# Identifier gros fichiers
du -sh /* | sort -rh | head -10
```

**Actions** :
1. Clean logs : `find /var/log -type f -mtime +7 -delete`
2. Clean Docker images : `docker system prune -a`
3. Rotate logs : `logrotate -f /etc/logrotate.conf`
4. Extend disk size (if persistent)

**Escalation** : Si <5% remaining ‚Üí Call SRE Team
```

#### Crit√®res Validation
- ‚úÖ Runbook complet (6 incidents)
- ‚úÖ Actions claires et actionnables
- ‚úÖ Escalation d√©finie

---

### TASK-P6-009 : Health Checks Production

**Agent** : Dev  
**Priorit√©** : CRITIQUE  
**Effort** : 0.5 jour  

#### Endpoints Health Checks

**Fichier** : `app/api/health.py`
```python
from fastapi import APIRouter, status
from pydantic import BaseModel
from typing import Dict
import asyncpg
import redis
import httpx
from datetime import datetime

router = APIRouter()

class HealthCheck(BaseModel):
    status: str
    timestamp: str
    version: str
    checks: Dict[str, str]

class ReadinessCheck(BaseModel):
    ready: bool
    checks: Dict[str, bool]

@router.get("/health", response_model=HealthCheck)
async def health():
    """
    Basic health check (liveness probe)
    
    Returns 200 if application is running
    """
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "checks": {
            "api": "ok"
        }
    }

@router.get("/health/ready", response_model=ReadinessCheck)
async def readiness():
    """
    Readiness check (readiness probe)
    
    Returns 200 if application is ready to accept traffic
    Checks:
    - Database connection
    - Redis connection
    - External APIs
    """
    checks = {}
    
    # Check Database
    try:
        pool = Database.get_pool()
        async with pool.acquire() as conn:
            await conn.fetchval("SELECT 1")
        checks["database"] = True
    except Exception as e:
        checks["database"] = False
    
    # Check Redis
    try:
        redis_client = Redis.get_client()
        await redis_client.ping()
        checks["redis"] = True
    except Exception as e:
        checks["redis"] = False
    
    # Check Firebase
    try:
        # Simple check (not actual API call)
        checks["firebase"] = True
    except Exception as e:
        checks["firebase"] = False
    
    # Check BANGE API
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://api.bange.com/health",
                timeout=2.0
            )
            checks["bange_api"] = response.status_code == 200
    except Exception:
        checks["bange_api"] = False
    
    # Ready if all critical checks pass
    ready = checks.get("database", False) and checks.get("redis", False)
    
    return {
        "ready": ready,
        "checks": checks
    }

@router.get("/health/live")
async def liveness():
    """
    Liveness check (liveness probe)
    
    Returns 200 if application is alive (not deadlocked)
    """
    return {"alive": True}
```

#### Configuration Kubernetes

**Fichier** : `k8s/backend-deployment.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: taxasge-production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: ghcr.io/taxasge/backend:latest
        ports:
        - containerPort: 8000
        
        # Liveness Probe (restart if dead)
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness Probe (remove from load balancer if not ready)
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        
        # Startup Probe (give time to startup)
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30  # 30 * 5 = 150s max startup time
```

#### Crit√®res Validation
- ‚úÖ 3 endpoints health checks (/health, /health/ready, /health/live)
- ‚úÖ Kubernetes probes configur√©es
- ‚úÖ Health checks test√©s

---

### TASK-P6-010 : Go-Live Checklist

**Agent** : Orchestrateur  
**Priorit√©** : CRITIQUE  
**Effort** : 0.5 jour  

#### Checklist Compl√®te Go-Live

```markdown
# GO-LIVE CHECKLIST - TAXASGE BACKEND

**Date Go-Live** : _____________  
**Responsable** : _____________

## 1. CI/CD Pipeline ‚úÖ

- [ ] GitHub Actions workflow configur√©
- [ ] Tests automatiques passants (coverage >85%)
- [ ] Build Docker automatique
- [ ] D√©ploiement staging automatique (branch develop)
- [ ] D√©ploiement production avec approbation (branch main)
- [ ] Rollback test√© et fonctionnel

## 2. Environnements ‚úÖ

- [ ] **Development** : Configur√© et fonctionnel
- [ ] **Staging** : URL https://api-staging.taxasge.com accessible
- [ ] **Production** : URL https://api.taxasge.com pr√™te
- [ ] Secrets configur√©s (DATABASE_URL, JWT_SECRET, etc.)
- [ ] ConfigMaps cr√©√©s pour chaque environnement
- [ ] CORS configur√© correctement

## 3. Database ‚úÖ

- [ ] PostgreSQL production provisionn√© (Cloud SQL)
- [ ] Backup automatique configur√© (daily)
- [ ] Point-in-time recovery activ√©
- [ ] Read replicas configur√©s (si n√©cessaire)
- [ ] Migrations Alembic test√©es en staging
- [ ] Migrations ex√©cut√©es en production
- [ ] Indexes cr√©√©s sur tables critiques
- [ ] Connection pooling configur√©

## 4. Monitoring ‚úÖ

- [ ] Prometheus d√©ploy√© et scraping backend
- [ ] Grafana d√©ploy√© et accessible
- [ ] 12 dashboards cr√©√©s et fonctionnels :
  - [ ] API Overview
  - [ ] Auth Module
  - [ ] Declarations Module
  - [ ] Payments Module
  - [ ] Webhooks Module
  - [ ] Agents Performance
  - [ ] Database Performance
  - [ ] Cache Performance
  - [ ] Notifications
  - [ ] OCR Performance
  - [ ] System Resources
  - [ ] Business Metrics
- [ ] M√©triques HTTP instrument√©es
- [ ] M√©triques business instrument√©es

## 5. Alertes ‚úÖ

- [ ] Alertmanager configur√©
- [ ] PagerDuty int√©gr√©
- [ ] Slack notifications configur√©es
- [ ] 7 alertes critiques configur√©es :
  - [ ] API Down
  - [ ] High Error Rate
  - [ ] Database Connection Lost
  - [ ] Webhook Processing Failures
  - [ ] High Latency
  - [ ] High CPU Usage
  - [ ] Disk Space Low
- [ ] Alertes test√©es (test alert envoy√©)
- [ ] Escalation policy d√©finie

## 6. S√©curit√© ‚úÖ

- [ ] JWT validation activ√©e
- [ ] RBAC enforcement activ√©
- [ ] Rate limiting configur√© (production)
- [ ] CORS configur√© (whitelist production)
- [ ] HTTPS activ√© (certificat SSL valide)
- [ ] Secrets jamais en clair (Kubernetes Secrets)
- [ ] Security scan pass√© (Bandit, Dependabot)
- [ ] Firewall rules configur√©es (whitelist IPs)

## 7. Performance ‚úÖ

- [ ] Load testing ex√©cut√© (1000 users)
- [ ] Endpoints critiques P95 <500ms
- [ ] Webhooks P95 <2s
- [ ] Auto-scaling configur√© (HPA)
- [ ] Resource limits d√©finis (CPU, Memory)
- [ ] CDN configur√© (Cloudflare)

## 8. Tests ‚úÖ

- [ ] Coverage tests >85%
- [ ] 20 sc√©narios E2E passants
- [ ] Tests int√©gration passants
- [ ] Tests s√©curit√© passants
- [ ] Tests performance passants
- [ ] Tests charge passants (spike, stress, endurance)
- [ ] Rapport QA final approuv√©

## 9. Documentation ‚úÖ

- [ ] Documentation d√©ploiement compl√®te
- [ ] Runbook incidents cr√©√© (6 incidents)
- [ ] API documentation Swagger √† jour
- [ ] README projet √† jour
- [ ] Architecture diagram cr√©√©
- [ ] Changelog maintenu

## 10. External Dependencies ‚úÖ

- [ ] BANGE API accessible
- [ ] BANGE webhooks configur√©s
- [ ] Firebase Storage configur√©
- [ ] Mailgun API key valide
- [ ] Twilio API key valide (SMS)
- [ ] Google Vision API key valide (OCR)
- [ ] All external APIs test√©s en production

## 11. Backup & Recovery ‚úÖ

- [ ] Database backup automatique (daily)
- [ ] Backup retention policy d√©finie (30 days)
- [ ] Restore test√© (backup ‚Üí restore ‚Üí verify)
- [ ] Disaster recovery plan document√©
- [ ] RTO (Recovery Time Objective) : <4h
- [ ] RPO (Recovery Point Objective) : <1h

## 12. Compliance ‚úÖ

- [ ] RGPD compliance v√©rifi√©e
- [ ] Data retention policy d√©finie
- [ ] Privacy policy √† jour
- [ ] Terms of service √† jour
- [ ] Audit logs activ√©s

## 13. Team Readiness ‚úÖ

- [ ] On-call rotation d√©finie
- [ ] PagerDuty escalation configur√©e
- [ ] Runbook partag√© avec √©quipe
- [ ] Training session effectu√©e
- [ ] Contact list √† jour (Dev, Ops, Support)

## 14. Go-Live Tasks ‚úÖ

- [ ] Communication clients (maintenance window si n√©cessaire)
- [ ] DNS configur√© (api.taxasge.com ‚Üí Load Balancer)
- [ ] SSL certificate install√©
- [ ] Monitoring dashboards ouverts
- [ ] PagerDuty activ√©
- [ ] Team on standby (1h post-deployment)

## 15. Post-Deployment ‚úÖ

- [ ] Smoke tests production passants
- [ ] Monitoring stable (15 min)
- [ ] No critical alerts
- [ ] Performance acceptable
- [ ] User feedback monitoring activ√©
- [ ] Incident channel cr√©√© (Slack #taxasge-incidents)

---

## Crit√®res Acceptation PRODUCTION

**TOUS LES ITEMS DOIVENT √äTRE COCH√âS** ‚úÖ

**Signatures** :

- **Lead Dev** : _______________ Date : ___________
- **DevOps** : _______________ Date : ___________
- **QA Lead** : _______________ Date : ___________
- **CTO** : _______________ Date : ___________

---

## GO / NO-GO DECISION

**D√©cision** : [ ] GO  [ ] NO-GO

**Justification** : _______________________________________

**Date Go-Live** : _______________________________________
```

#### Crit√®res Validation
- ‚úÖ Checklist compl√®te (15 sections, 100+ items)
- ‚úÖ Tous items critiques coch√©s
- ‚úÖ Signatures obtenues

---

## üìä KPIs Phase 6

| M√©trique | Avant | Apr√®s | Objectif |
|----------|-------|-------|----------|
| CI/CD Pipeline | ‚ùå | ‚úÖ | Automatique |
| Environnements | 1 (dev) | 3 (dev/staging/prod) | 3 |
| Dashboards Grafana | 0 | 12 | 12 |
| Alertes configur√©es | 0 | 7 | 7 |
| Uptime SLA | N/A | 99.9% | 99.9% |
| Deploy Time | Manual | <5min | <5min |
| Rollback Time | N/A | <2min | <2min |

---

## ‚è±Ô∏è Timeline Phase 6

| Jour | T√¢ches | Agent | Statut |
|------|--------|-------|--------|
| J1 | TASK-P6-001 GitHub Actions | Dev | ‚úÖ |
| J1 | TASK-P6-002 Environnements | Dev | ‚úÖ |
| J2 | TASK-P6-003 Migrations Alembic | Dev | ‚úÖ |
| J3 | TASK-P6-004 Prometheus + Grafana | Dev | ‚úÖ |
| J4 | TASK-P6-005 12 Dashboards | Dev | ‚úÖ |
| J4 | TASK-P6-006 Alertes PagerDuty | Dev | ‚úÖ |
| J5 | TASK-P6-007 Doc D√©ploiement | Doc | ‚úÖ |
| J5 | TASK-P6-008 Runbook | Doc | ‚úÖ |
| J5 | TASK-P6-009 Health Checks | Dev | ‚úÖ |
| J5 | TASK-P6-010 Go-Live Checklist | Orchestrateur | ‚úÖ |

**Total** : 5 jours (1 semaine)

---

## üéâ FIN DU PROJET

**Backend TaxasGE est PRODUCTION READY** ‚úÖ

**Prochaine action** : GO-LIVE ! üöÄ

---

**Statut Final** : ‚úÖ PR√äT POUR PRODUCTION  
**Date** : 2025-10-20  
**Version** : 1.0.0
