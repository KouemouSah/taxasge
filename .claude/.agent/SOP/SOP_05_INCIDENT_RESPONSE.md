# SOP 5 : INCIDENT RESPONSE

**Fr√©quence** : √Ä la d√©tection (24/7)  
**Dur√©e** : Variable selon s√©v√©rit√©  
**Participants** : On-Call Engineer + Escalation selon besoin

---

## OBJECTIF

R√©pondre rapidement aux incidents production pour :
- ‚úÖ Minimiser downtime
- ‚úÖ R√©duire impact users
- ‚úÖ Communiquer clairement
- ‚úÖ Apprendre et am√©liorer

---

## CLASSIFICATION INCIDENTS

### üî¥ SEV-1 (CRITIQUE)
**Crit√®res** :
- Service compl√®tement down (>90% users impact√©s)
- Perte de donn√©es
- Breach s√©curit√©
- Revenus bloqu√©s (>1M XAF/h perdu)

**Exemples** :
- "API retourne 500 sur tous endpoints"
- "Database connexion perdue"
- "Tous webhooks BANGE √©chouent ‚Üí 0 paiements"
- "Faille s√©curit√© exploit√©e activement"

**SLA** : Acknowledge <5 min, Resolve <2h
**Escalation** : Imm√©diate (CTO + Team enti√®re)

---

### üü† SEV-2 (MAJEUR)
**Crit√®res** :
- Feature critique down (20-90% users impact√©s)
- Performance s√©v√®rement d√©grad√©e
- Perte partielle service
- Revenus impact√©s mais pas bloqu√©s

**Exemples** :
- "Login impossible pour users Gmail"
- "Upload documents √©choue √† 80%"
- "P95 latency >3s (normal: 500ms)"
- "Agents ne voient pas la queue"

**SLA** : Acknowledge <15 min, Resolve <4h
**Escalation** : Lead Dev + On-call

---

### üü° SEV-3 (MINEUR)
**Crit√®res** :
- Feature non-critique impact√©e (<20% users)
- Performance l√©g√®rement d√©grad√©e
- Workaround existe
- Impact limit√©

**Exemples** :
- "Notifications email delayed (1h de retard)"
- "Dashboard admin stats incorrectes"
- "Export CSV ne fonctionne pas"
- "OCR quality faible (60% au lieu de 80%)"

**SLA** : Acknowledge <1h, Resolve <24h
**Escalation** : Standard (pas d'escalation imm√©diate)

---

## ON-CALL ROTATION

### Schedule (24/7)
```
Week 1 : Agent DEV (primary) + Orchestrateur (backup)
Week 2 : Orchestrateur (primary) + Agent DEV (backup)

Rotation : Lundi 9h ‚Üí Lundi 9h (1 semaine)
```

### Responsibilities
```markdown
- [ ] R√©pondre aux alerts PagerDuty (<5 min SEV-1, <15 min SEV-2)
- [ ] Investiguer et diagnostiquer incident
- [ ] Appliquer fix ou workaround
- [ ] Communiquer statut (Slack, status page)
- [ ] Escalate si n√©cessaire
- [ ] √âcrire post-mortem (SEV-1/SEV-2)
```

### Handoff Process
```markdown
## On-Call Handoff - Lundi 9h

**Sortant** (Agent DEV) :
‚úÖ No active incidents
‚ö†Ô∏è Watch items :
- Database replica lag (slight delay, monitoring)
- BANGE API showing intermittent slowness (no impact yet)

üìù Incidents derni√®re semaine :
- INC-001 : Webhook processing spike (resolved)
- INC-002 : Memory leak pod-3 (restarted)

üéØ Upcoming :
- Maintenance database mercredi 22h (planned)

**Entrant** (Orchestrateur) :
‚úÖ Acknowledge handoff
‚úÖ Verified PagerDuty configured
‚úÖ Verified access to all systems
‚úÖ Read runbook and recent incidents

**Status** : Handoff complete ‚úÖ
```

---

## PROC√âDURE R√âPONSE INCIDENT

### PHASE 1 : D√âTECTION & TRIAGE (0-5 min)

#### 1.1 Alert Re√ßue (PagerDuty)
```
üì± PagerDuty Alert
Severity: SEV-1
Title: "API Down - All endpoints returning 500"
Time: 2025-10-20 22:15
```

#### 1.2 Acknowledge (< 5 min)
```bash
# Click "Acknowledge" dans PagerDuty
# ‚Üí Stop alarme, assign incident √† soi
```

#### 1.3 √âvaluer S√©v√©rit√©
```
Q1: Service down compl√®tement ? ‚Üí SEV-1
Q2: Feature critique impact√©e ? ‚Üí SEV-2
Q3: Impact limit√©, workaround existe ? ‚Üí SEV-3
```

#### 1.4 Cr√©er Incident Slack Channel
```
# Cr√©er channel
/create-channel #incident-2025-10-20-001

# Post initial status
üì¢ INCIDENT DECLARED

**Severity** : SEV-1 üî¥
**Title** : API Down - All endpoints returning 500
**Status** : Investigating
**Impact** : 100% users cannot access application
**Started** : 2025-10-20 22:15

**On-Call** : @orchestrateur
**Link** : https://pagerduty.com/incidents/ABC123

Investigation in progress...
```

---

### PHASE 2 : INVESTIGATION (5-30 min)

#### 2.1 Check Health & Logs
```bash
# 1. Check pods status
kubectl get pods -n taxasge-production

# Output example
NAME                    READY   STATUS    RESTARTS   AGE
backend-7d4b8c-abc      0/1     Error     5          10m
backend-7d4b8c-def      0/1     Error     5          10m
backend-7d4b8c-ghi      0/1     Error     5          10m

# 2. Check logs (derni√®res erreurs)
kubectl logs backend-7d4b8c-abc --tail=50

# Output example
ERROR: Unable to connect to database
psycopg2.OperationalError: could not connect to server
```

#### 2.2 Check External Dependencies
```bash
# Database
psql $DATABASE_URL -c "SELECT 1"
# ‚Üí Error: Connection refused

# Redis
redis-cli -h redis.taxasge.com ping
# ‚Üí PONG (OK)

# BANGE API
curl https://api.bange.com/health
# ‚Üí 200 OK
```

#### 2.3 Check Metrics (Grafana)
```
# Open dashboards
https://grafana.taxasge.com/d/api-overview

Observations:
- Requests/sec: 0 (dropped to 0 at 22:15)
- Error rate: 100%
- Database connections: 0 (normally ~10)
```

#### 2.4 Root Cause Identified
```
üîç ROOT CAUSE FOUND

Database Cloud SQL instance is down.

Evidence:
- All pods failing database connection
- Cloud SQL status page shows "Incident"
- No changes deployed recently

Action: Escalate to GCP Support
```

---

### PHASE 3 : MITIGATION (30-60 min)

#### 3.1 Apply Workaround (if possible)
```bash
# Example: Switch to failover database
kubectl set env deployment/backend \
  DATABASE_URL=$DATABASE_URL_REPLICA

kubectl rollout restart deployment/backend
```

#### 3.2 Escalate (si n√©cessaire)
```
SEV-1 ‚Üí Escalate imm√©diatement

Escalation List:
1. CTO (@cto-slack)
2. GCP Support (ticket + phone)
3. BANGE Support (si webhook issue)
4. Team Dev complete (via Slack)
```

#### 3.3 Communication Updates (Every 15 min)
```markdown
üì¢ UPDATE #1 - T+15 min

**Status** : Investigating
**Root Cause** : Database Cloud SQL down
**Action** : 
- Escalated to GCP Support (ticket #123456)
- Attempting failover to replica database
**ETA** : 30 min

100% users still impacted. Working on resolution.
```

```markdown
üì¢ UPDATE #2 - T+30 min

**Status** : Mitigating
**Progress** :
‚úÖ GCP confirmed incident on their side
‚úÖ Failover to replica database completed
‚è≥ Testing application

**ETA** : 15 min

Service should be restored shortly.
```

---

### PHASE 4 : R√âSOLUTION (60-120 min)

#### 4.1 Verify Fix
```bash
# 1. Check pods healthy
kubectl get pods -n taxasge-production
# ‚Üí All Running

# 2. Check health endpoint
curl https://api.taxasge.com/health
# ‚Üí 200 OK

# 3. Test critical endpoints
curl https://api.taxasge.com/api/v1/fiscal-services?limit=1
# ‚Üí 200 OK

# 4. Check metrics
# ‚Üí Requests/sec back to normal (~1000)
# ‚Üí Error rate <1%
```

#### 4.2 Resolve Incident
```bash
# Mark resolved in PagerDuty
# Status: Resolved
```

#### 4.3 Communication R√©solution
```markdown
‚úÖ INCIDENT RESOLVED

**Duration** : 1h 15min (22:15 - 23:30)
**Impact** : 100% users unable to access application
**Root Cause** : GCP Cloud SQL instance failure
**Resolution** : Failover to replica database

**Timeline** :
- 22:15 : Incident detected (PagerDuty alert)
- 22:20 : Root cause identified (database down)
- 22:25 : Escalated to GCP Support
- 22:45 : Failover initiated
- 23:00 : Testing completed
- 23:30 : Service fully restored

**Next Steps** :
- GCP investigating root cause
- Post-mortem scheduled (tomorrow 10h)
- Improve alerting (detect DB issues faster)

Thank you for your patience. Service is now stable.
```

---

### PHASE 5 : POST-INCIDENT

#### 5.1 Monitoring (1h post-resolution)
```
Monitor dashboards for 1h to ensure stability:
- No new alerts
- Metrics stable
- No error spikes
```

#### 5.2 Post-Mortem (SEV-1/SEV-2) - Next Day
```markdown
# POST-MORTEM : INC-2025-10-20-001

**Date** : 2025-10-20
**Severity** : SEV-1 üî¥
**Duration** : 1h 15min
**Impact** : 100% users (5,000 active users affected)

## Summary
Complete service outage due to GCP Cloud SQL instance failure.

## Timeline (All times GMT+1)
| Time | Event |
|------|-------|
| 22:15 | PagerDuty alert triggered |
| 22:17 | Incident acknowledged by On-Call |
| 22:20 | Root cause identified (database down) |
| 22:25 | Escalated to GCP Support |
| 22:30 | Attempted automatic failover (failed) |
| 22:45 | Manual failover initiated |
| 23:00 | Testing application on replica |
| 23:30 | Service fully restored |

## Root Cause
GCP Cloud SQL primary instance experienced hardware failure (confirmed by GCP).

## Impact
- **Users** : 100% unable to access application (5,000 users)
- **Duration** : 1h 15min total downtime
- **Revenue** : Estimated 200 declarations delayed (~50M XAF)
- **SLA** : Breached (99.9% ‚Üí 99.8% monthly uptime)

## Resolution
Manual failover to read replica promoted to primary.

## What Went Well ‚úÖ
- Alert triggered immediately (PagerDuty)
- On-Call responded quickly (<5 min)
- Root cause identified fast (5 min)
- Communication clear and frequent (every 15 min)
- Failover process worked as designed

## What Went Wrong ‚ùå
- Automatic failover didn't trigger (should be automatic)
- No early warning (database health deteriorating)
- Replica not fully synced (30s lag, minor data loss possible)
- Documentation incomplete (failover steps not clear)

## Action Items
| # | Action | Owner | Due Date | Status |
|---|--------|-------|----------|--------|
| 1 | Fix automatic failover (investigate why failed) | Orchestrateur | 2025-10-22 | üü° In Progress |
| 2 | Add database health monitoring alerts | Agent DEV | 2025-10-21 | ‚úÖ Done |
| 3 | Document failover procedure (step-by-step) | Agent DOC | 2025-10-21 | ‚úÖ Done |
| 4 | Setup sync replication (zero lag) | Orchestrateur | 2025-10-25 | üî¥ Planned |
| 5 | Schedule GCP meeting (review incident) | Orchestrateur | 2025-10-23 | üü° Scheduled |

## Lessons Learned
1. **Redundancy is critical** : Replica saved us, but automatic failover failed
2. **Monitoring needs improvement** : Should alert before complete failure
3. **Documentation matters** : Clear runbook would have saved 15 min
4. **Communication worked well** : Status updates helped manage expectations

## Prevention
- **Monitoring** : Add alert for database connection pool exhaustion
- **Automation** : Fix automatic failover mechanism
- **Testing** : Schedule quarterly disaster recovery drills
- **Documentation** : Maintain updated runbooks

---

**Reviewed by** : CTO, Lead Dev, On-Call Engineer
**Date** : 2025-10-21
```

---

## COMMUNICATION GUIDELINES

### Internal (Slack #incidents)
```
‚úÖ Frequency : Every 15 min minimum (SEV-1/SEV-2)
‚úÖ Audience : Team Dev, Ops, Management
‚úÖ Content : Technical details, actions, ETA
‚úÖ Format : Structured updates (see templates)
```

### External (Status Page)
```
‚úÖ Frequency : Every 30 min (SEV-1), 1h (SEV-2)
‚úÖ Audience : Users, customers
‚úÖ Content : Impact, status, ETA (no technical jargon)
‚úÖ Format : Simple language, empathetic

Example:
"We're experiencing issues with our service. 
Our team is actively working on a fix. 
We apologize for the inconvenience and will 
provide updates every 30 minutes."
```

### Executive (Email/Phone)
```
‚úÖ Frequency : Initial alert + resolution (SEV-1), daily summary (SEV-2)
‚úÖ Audience : CTO, CEO, Leadership
‚úÖ Content : Business impact, financial impact, resolution plan
‚úÖ Format : Executive summary (1 page max)
```

---

## RUNBOOK QUICK LINKS

**Common Incidents** :
1. [API Down](link-to-runbook-1)
2. [Database Connection Lost](link-to-runbook-2)
3. [High Error Rate](link-to-runbook-3)
4. [Webhook Processing Failures](link-to-runbook-4)
5. [High Latency](link-to-runbook-5)
6. [Memory Leak](link-to-runbook-6)

**Access Links** :
- [PagerDuty](https://taxasge.pagerduty.com)
- [Grafana](https://grafana.taxasge.com)
- [Kubernetes Dashboard](https://k8s.taxasge.com)
- [GCP Console](https://console.cloud.google.com)
- [Slack #incidents](https://taxasge.slack.com/archives/incidents)

---

## INCIDENT SEVERITY MATRIX

| Criteria | SEV-1 | SEV-2 | SEV-3 |
|----------|-------|-------|-------|
| Users Impacted | >90% | 20-90% | <20% |
| Revenue Impact | >1M XAF/h | 100K-1M XAF/h | <100K XAF/h |
| Acknowledge SLA | <5 min | <15 min | <1h |
| Resolve SLA | <2h | <4h | <24h |
| Update Frequency | 15 min | 30 min | 1h |
| Escalation | Immediate | If not resolved 2h | Standard |
| Post-Mortem | Required | Required | Optional |

---

## ANTI-PATTERNS

‚ùå **Panic & guess** ‚Üí Rend situation pire
‚úÖ Solution : Follow runbook, methodical investigation

‚ùå **No communication** ‚Üí Users/Team dans le noir
‚úÖ Solution : Update every 15 min minimum (SEV-1)

‚ùå **Fix without understanding** ‚Üí Probl√®me revient
‚úÖ Solution : Root cause analysis AVANT fix

‚ùå **Skip post-mortem** ‚Üí Ne pas apprendre
‚úÖ Solution : Post-mortem obligatoire SEV-1/SEV-2

‚ùå **Blame culture** ‚Üí Team afraid to report issues
‚úÖ Solution : Blameless post-mortems, focus on process

---

## METRICS & KPIs

| M√©trique | Target | Actuel |
|----------|--------|--------|
| MTTA (Mean Time To Acknowledge) | <5 min (SEV-1) | 3 min ‚úÖ |
| MTTR (Mean Time To Resolve) | <2h (SEV-1) | 1h 15min ‚úÖ |
| Incidents/month | <5 | 2 ‚úÖ |
| SEV-1 incidents/quarter | <3 | 1 ‚úÖ |
| Post-mortem completion | 100% | 100% ‚úÖ |
| Action items closed | >80% in 30 days | 85% ‚úÖ |

---

**Version** : 1.0  
**Derni√®re mise √† jour** : 2025-10-20  
**Propri√©taire** : Orchestrateur
