name: 📊 TaxasGE Historical Context Mapper

on:
  workflow_dispatch:
    inputs:
      analyze_infrastructure:
        description: 'Analyser infrastructure existante'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'
      analyze_codebase:
        description: 'Analyser codebase et commits'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'
      generate_report:
        description: 'Générer rapport complet'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'

permissions:
  contents: write  # Pour auto-commit dans docs/
  actions: read
  issues: read

jobs:
  historical-mapper:
    name: 📊 Historical Context Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for analysis
          
      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: 📦 Install dependencies
        run: |
          pip install requests python-dateutil pathlib
          
      - name: 🔍 Create Historical Analysis Script
        run: |
          cat > historical_mapper.py << 'EOF'
          import os
          import json
          import requests
          import subprocess
          from datetime import datetime, timedelta
          from pathlib import Path
          import re
          
          class TaxasGEHistoricalMapper:
              def __init__(self, token, repo_owner, repo_name):
                  self.token = token
                  self.repo_owner = repo_owner
                  self.repo_name = repo_name
                  self.base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}"
                  self.headers = {
                      "Authorization": f"token {token}",
                      "Accept": "application/vnd.github.v3+json",
                      "User-Agent": "TaxasGE-Historical-Mapper/1.0"
                  }
                  
              def analyze_infrastructure(self):
                  """Analyse l'infrastructure existante"""
                  print("🔍 Analyse de l'infrastructure existante...")
                  
                  infrastructure = {
                      "github_actions": self.analyze_workflows(),
                      "firebase_integration": self.detect_firebase_config(),
                      "supabase_integration": self.detect_supabase_config(),
                      "sonarqube_integration": self.detect_sonarqube_config(),
                      "ai_model": self.detect_ai_model(),
                      "dashboard": self.analyze_dashboard(),
                      "mobile_app": self.analyze_mobile_structure()
                  }
                  
                  return infrastructure
              
              def analyze_workflows(self):
                  """Analyse les workflows GitHub Actions"""
                  workflows_dir = Path('.github/workflows')
                  if not workflows_dir.exists():
                      return {"status": "not_found", "completion": 0}
                  
                  workflows = list(workflows_dir.glob('*.yml')) + list(workflows_dir.glob('*.yaml'))
                  workflow_analysis = {
                      "total_workflows": len(workflows),
                      "workflows": [],
                      "status": "configured" if workflows else "missing",
                      "completion": 90 if workflows else 0
                  }
                  
                  for workflow in workflows:
                      workflow_analysis["workflows"].append({
                          "name": workflow.name,
                          "path": str(workflow),
                          "exists": True
                      })
                  
                  # Check for specific critical workflows
                  critical_workflows = [
                      "status-dashboard", "backend-ci", "create-issues", 
                      "mobile-ci", "deploy"
                  ]
                  
                  found_critical = 0
                  for critical in critical_workflows:
                      for workflow in workflows:
                          if critical in workflow.name.lower():
                              found_critical += 1
                              break
                  
                  workflow_analysis["critical_coverage"] = (found_critical / len(critical_workflows)) * 100
                  
                  return workflow_analysis
              
              def detect_firebase_config(self):
                  """Détecte la configuration Firebase"""
                  firebase_indicators = [
                      "firebase.json",
                      ".firebaserc", 
                      "packages/mobile/google-services.json",
                      "packages/mobile/GoogleService-Info.plist"
                  ]
                  
                  found_configs = []
                  for indicator in firebase_indicators:
                      if Path(indicator).exists():
                          found_configs.append(indicator)
                  
                  # Check for Firebase imports in code
                  firebase_imports = self.search_in_codebase([
                      "from firebase",
                      "import firebase",
                      "@firebase/",
                      "firebase/app"
                  ])
                  
                  completion = 0
                  if found_configs:
                      completion = 70
                  if firebase_imports:
                      completion = 95  # User confirmed it's working
                  
                  return {
                      "status": "configured" if completion > 0 else "missing",
                      "completion": completion,
                      "config_files": found_configs,
                      "code_integration": len(firebase_imports) > 0,
                      "validation": "user_confirmed_working" if completion >= 95 else "needs_validation"
                  }
              
              def detect_supabase_config(self):
                  """Détecte la configuration Supabase"""
                  supabase_indicators = [
                      ".env",
                      ".env.local", 
                      "supabase/config.toml",
                      "packages/backend/app/database/"
                  ]
                  
                  found_configs = []
                  for indicator in supabase_indicators:
                      if Path(indicator).exists():
                          found_configs.append(indicator)
                  
                  # Check for Supabase imports
                  supabase_imports = self.search_in_codebase([
                      "supabase",
                      "@supabase/",
                      "createClient"
                  ])
                  
                  return {
                      "status": "configured",  # User confirmed
                      "completion": 95,  # User confirmed tested and validated
                      "config_files": found_configs,
                      "code_integration": len(supabase_imports) > 0,
                      "validation": "user_confirmed_tested",
                      "database_schema": "547_taxes_configured"
                  }
              
              def detect_sonarqube_config(self):
                  """Détecte la configuration SonarQube"""
                  sonar_indicators = [
                      "sonar-project.properties",
                      ".sonarcloud.properties",
                      "sonarqube.yml",
                      "sonar-scanner.properties"
                  ]
                  
                  found_configs = []
                  for indicator in sonar_indicators:
                      if Path(indicator).exists():
                          found_configs.append(indicator)
                  
                  # Check workflows for SonarQube
                  sonar_in_workflows = self.search_in_directory(".github/workflows", [
                      "sonarqube",
                      "sonar",
                      "quality"
                  ])
                  
                  return {
                      "status": "configured",  # User confirmed
                      "completion": 95,  # User confirmed tested and validated
                      "config_files": found_configs,
                      "workflow_integration": len(sonar_in_workflows) > 0,
                      "validation": "user_confirmed_tested"
                  }
              
              def detect_ai_model(self):
                  """Détecte le modèle IA TensorFlow Lite"""
                  model_path = "packages/mobile/src/assets/ml/taxasge_model.tflite"
                  model_exists = Path(model_path).exists()
                  
                  # Search for AI/ML related code
                  ai_patterns = [
                      "tensorflow",
                      "tflite", 
                      "@tensorflow/tfjs-react-native",
                      "ml-kit",
                      "react-native-fs"
                  ]
                  
                  ai_code = self.search_in_codebase(ai_patterns)
                  
                  return {
                      "status": "ready",
                      "completion": 95,  # Pipeline completed externally, user confirmed
                      "model_file": model_path,
                      "model_exists": model_exists,
                      "model_size": "0.41MB",
                      "accuracy": "100%_on_62k_questions",
                      "languages": ["ES", "FR", "EN"],
                      "data_pipeline": {
                          "status": "completed_externally",  # Scripts not in repo, but user confirmed
                          "pdf_extraction": "validated_user_confirmed",
                          "json_standardization": "validated_user_confirmed", 
                          "multilingual_transformation": "validated_user_confirmed",
                          "training": "completed_user_confirmed",
                          "corpus_generation": "62k_questions_completed",
                          "validation": "user_confirmed_working",
                          "note": "Pipeline executed externally, scripts not in repository"
                      },
                      "deployment_pending": "mobile_integration",
                      "code_integration": len(ai_code) > 0,
                      "critical_path": True
                  }
              
              def analyze_dashboard(self):
                  """Analyse le dashboard de monitoring"""
                  dashboard_files = [
                      "docs/index.html",
                      "index.html",
                      "dashboard.html"
                  ]
                  
                  found_dashboard = None
                  for dashboard in dashboard_files:
                      if Path(dashboard).exists():
                          found_dashboard = dashboard
                          break
                  
                  dashboard_features = []
                  if found_dashboard:
                      content = Path(found_dashboard).read_text(encoding='utf-8')
                      if "TaxasGE Dashboard" in content:
                          dashboard_features.append("branding")
                      if "progress-bar" in content:
                          dashboard_features.append("progress_bars")
                      if "kpi-card" in content:
                          dashboard_features.append("kpi_cards")
                      if "status-dot" in content:
                          dashboard_features.append("status_indicators")
                  
                  return {
                      "status": "production_ready" if found_dashboard else "missing",
                      "completion": 90 if found_dashboard else 0,
                      "file": found_dashboard,
                      "features": dashboard_features,
                      "phases_completed": ["Phase_1_Layout", "Phase_2_Metrics", "Phase_3_Optimization"],
                      "github_pages": "configured",
                      "remaining": ["final_github_pages_config"]
                  }
              
              def analyze_mobile_structure(self):
                  """Analyse la structure de l'app mobile"""
                  mobile_indicators = [
                      "packages/mobile/package.json",
                      "packages/mobile/app.json",
                      "packages/mobile/metro.config.js",
                      "packages/mobile/src/"
                  ]
                  
                  found_mobile = []
                  for indicator in mobile_indicators:
                      if Path(indicator).exists():
                          found_mobile.append(indicator)
                  
                  # Check for React Native specific files
                  rn_patterns = [
                      "react-native",
                      "@react-navigation",
                      "expo",
                      "react-native-vector-icons"
                  ]
                  
                  rn_integration = self.search_in_directory("packages/mobile", rn_patterns)
                  
                  return {
                      "status": "structured" if found_mobile else "missing",
                      "completion": 60,  # Structure + Firebase ready, pending AI integration
                      "structure_files": found_mobile,
                      "react_native_integration": len(rn_integration) > 0,
                      "firebase_ready": True,  # User confirmed
                      "ai_model_ready": True,  # Model exists
                      "pending": ["ai_model_integration", "ui_polish", "testing"],
                      "estimated_completion": "2_weeks"
                  }
              
              def analyze_codebase_history(self):
                  """Analyse l'historique des commits"""
                  print("📚 Analyse de l'historique des commits...")
                  
                  try:
                      # Get commit history
                      result = subprocess.run([
                          'git', 'log', '--oneline', '--since=6 months ago'
                      ], capture_output=True, text=True)
                      
                      commits = result.stdout.strip().split('\n') if result.stdout else []
                      
                      # Analyze commit patterns
                      patterns = {
                          "infrastructure": ["firebase", "supabase", "sonar", "workflow", "ci", "deploy"],
                          "backend": ["api", "backend", "fastapi", "endpoint", "database"],
                          "mobile": ["mobile", "react-native", "expo", "app", "android", "ios"],
                          "dashboard": ["dashboard", "monitoring", "status", "ui", "frontend"],
                          "ai": ["ai", "ml", "model", "tensorflow", "training", "data"]
                      }
                      
                      commit_analysis = {
                          "total_commits": len([c for c in commits if c]),
                          "timespan": "6_months",
                          "activity_by_phase": {}
                      }
                      
                      for phase, keywords in patterns.items():
                          matching_commits = []
                          for commit in commits:
                              if any(keyword in commit.lower() for keyword in keywords):
                                  matching_commits.append(commit)
                          
                          commit_analysis["activity_by_phase"][phase] = {
                              "commits": len(matching_commits),
                              "samples": matching_commits[:3]  # First 3 examples
                          }
                      
                      return commit_analysis
                      
                  except Exception as e:
                      print(f"⚠️  Erreur analyse commits: {e}")
                      return {"error": str(e)}
              
              def search_in_codebase(self, patterns):
                  """Recherche des patterns dans le codebase"""
                  matches = []
                  
                  # Common code directories
                  search_dirs = [
                      "packages/backend",
                      "packages/mobile", 
                      "src/",
                      "app/",
                      "."
                  ]
                  
                  for search_dir in search_dirs:
                      if Path(search_dir).exists():
                          matches.extend(self.search_in_directory(search_dir, patterns))
                  
                  return matches
              
              def search_in_directory(self, directory, patterns):
                  """Recherche des patterns dans un répertoire"""
                  matches = []
                  
                  try:
                      for pattern in patterns:
                          result = subprocess.run([
                              'grep', '-r', '-l', pattern, directory
                          ], capture_output=True, text=True, timeout=30)
                          
                          if result.returncode == 0:
                              files = result.stdout.strip().split('\n')
                              matches.extend(files)
                  except Exception:
                      pass  # Ignore grep errors
                  
                  return list(set(matches))  # Remove duplicates
              
              def generate_historical_context(self):
                  """Génère le contexte historique complet"""
                  print("🚀 Génération du contexte historique TaxasGE...")
                  
                  # Collect all analyses
                  infrastructure = self.analyze_infrastructure() if os.getenv("ANALYZE_INFRASTRUCTURE", "true") == "true" else {}
                  codebase_history = self.analyze_codebase_history() if os.getenv("ANALYZE_CODEBASE", "true") == "true" else {}
                  
                  # Calculate overall progress
                  phase_completions = {
                      "phase_0_infrastructure": 95,  # User confirmed all services working
                      "phase_1_backend": 70,   # Structure + API + Supabase, pending tests
                      "phase_2_mobile": 60,    # Structure + Firebase + AI model ready, pending integration
                      "phase_3_dashboard": 90, # Phases 1-3 completed, minor config pending
                      "phase_4_production": 20, # Planning phase
                      "phase_5_business": 10   # Planning phase
                  }
                  
                  overall_progress = sum(phase_completions.values()) / len(phase_completions)
                  
                  historical_context = {
                      "metadata": {
                          "generated_at": datetime.now().isoformat(),
                          "repository": f"{self.repo_owner}/{self.repo_name}",
                          "analysis_type": "historical_mapping",
                          "overall_progress": round(overall_progress, 1)
                      },
                      "project_status": {
                          "current_phase": "mobile_integration_critical_path",
                          "confidence_level": "high",
                          "foundation_strength": "excellent",
                          "risk_level": "low"
                      },
                      "phases": {
                          "phase_0_infrastructure": {
                              "title": "🛠️ Infrastructure & AI Pipeline", 
                              "completion": phase_completions["phase_0_infrastructure"],
                              "status": "completed",
                              "validation": "user_confirmed_all_services_working",
                              "components": infrastructure
                          },
                          "phase_1_backend": {
                              "title": "🐍 Backend & API",
                              "completion": phase_completions["phase_1_backend"], 
                              "status": "in_progress",
                              "critical_path": False
                          },
                          "phase_2_mobile": {
                              "title": "📱 Mobile Development",
                              "completion": phase_completions["phase_2_mobile"],
                              "status": "in_progress", 
                              "critical_path": True,
                              "blocking": ["ai_model_integration"]
                          },
                          "phase_3_dashboard": {
                              "title": "🌐 Dashboard & Monitoring",
                              "completion": phase_completions["phase_3_dashboard"],
                              "status": "nearly_complete",
                              "critical_path": False
                          },
                          "phase_4_production": {
                              "title": "🚀 Production Ready",
                              "completion": phase_completions["phase_4_production"],
                              "status": "planned",
                              "depends_on": ["phase_2_mobile"]
                          },
                          "phase_5_business": {
                              "title": "💳 Business Integration",
                              "completion": phase_completions["phase_5_business"],
                              "status": "planned", 
                              "depends_on": ["phase_4_production"]
                          }
                      },
                      "codebase_analysis": codebase_history,
                      "recommendations": {
                          "immediate_focus": ["mobile_ai_integration", "dashboard_github_pages_config"],
                          "next_phase": "production_readiness",
                          "estimated_completion": "8_weeks",
                          "confidence": "high_due_to_solid_foundation"
                      }
                  }
                  
                  return historical_context
          
          def main():
              print("📊 TaxasGE Historical Context Mapper")
              print("=====================================")
              
              # Configuration
              token = os.getenv("GITHUB_TOKEN")
              repo_owner = os.getenv("REPO_OWNER")
              repo_name = os.getenv("REPO_NAME")
              
              if not all([token, repo_owner, repo_name]):
                  print("❌ Variables d'environnement manquantes")
                  return
              
              # Initialize mapper
              mapper = TaxasGEHistoricalMapper(token, repo_owner, repo_name)
              
              # Generate historical context
              historical_context = mapper.generate_historical_context()
              
              # Save to file
              output_file = "historical-context.json"
              with open(output_file, 'w', encoding='utf-8') as f:
                  json.dump(historical_context, f, indent=2, ensure_ascii=False)
              
              print(f"✅ Contexte historique généré: {output_file}")
              
              # Display summary
              print(f"\n📊 Résumé TaxasGE:")
              print(f"   Progression globale: {historical_context['metadata']['overall_progress']}%")
              print(f"   Phase critique: {historical_context['project_status']['current_phase']}")
              print(f"   Niveau confiance: {historical_context['project_status']['confidence_level']}")
              print(f"   Estimation completion: {historical_context['recommendations']['estimated_completion']}")
              
              for phase_key, phase_data in historical_context['phases'].items():
                  status_icon = "✅" if phase_data['completion'] >= 90 else "🔄" if phase_data['completion'] >= 50 else "📋"
                  print(f"   {status_icon} {phase_data['title']}: {phase_data['completion']}%")
          
          if __name__ == "__main__":
              main()
          EOF
          
      - name: 📊 Execute Historical Analysis
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO_OWNER: ${{ github.repository_owner }}
          REPO_NAME: ${{ github.event.repository.name }}
          ANALYZE_INFRASTRUCTURE: ${{ github.event.inputs.analyze_infrastructure }}
          ANALYZE_CODEBASE: ${{ github.event.inputs.analyze_codebase }}
          GENERATE_REPORT: ${{ github.event.inputs.generate_report }}
        run: |
          python historical_mapper.py
          
      - name: 📄 Upload Historical Context
        uses: actions/upload-artifact@v4
        with:
          name: taxasge-historical-context
          path: historical-context.json
          retention-days: 30
          
      - name: 📄 Auto-commit to docs/
        if: github.event.inputs.generate_report == 'true'
        run: |
          # Create docs directory if it doesn't exist
          mkdir -p docs
          
          # Copy historical context to docs
          cp historical-context.json docs/historical-context.json
          
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action - Historical Context"
          
          # Add and commit if changes exist
          git add docs/historical-context.json
          if git diff --staged --quiet; then
            echo "ℹ️  No changes to commit"
          else
            git commit -m "📊 Update TaxasGE historical context analysis
            
            - Overall progress: $(python3 -c 'import json; data=json.load(open(\"historical-context.json\")); print(f\"{data[\"metadata\"][\"overall_progress\"]}%\")')
            - Current phase: $(python3 -c 'import json; data=json.load(open(\"historical-context.json\")); print(data[\"project_status\"][\"current_phase\"])')
            - Auto-generated by Historical Context Mapper workflow"
            git push
            echo "✅ Historical context committed to docs/historical-context.json"
          fi
          
      - name: 📊 Display Analysis Results
        if: success()
        run: |
          echo "🎯 Analyse historique TaxasGE terminée"
          echo "📄 Artifact: historical-context.json disponible"
          echo "🔄 Prêt pour ÉTAPE 2: Retroactive Project Builder"

          # Show file content summary
          if [ -f "historical-context.json" ]; then
            echo "📊 Preview du contexte:"
            python3 -c "
          import json
          with open('historical-context.json', 'r') as f:
          data = json.load(f)
          print(f'Progression: {data[\"metadata\"][\"overall_progress\"]}%')
          print(f'Phase critique: {data[\"project_status\"][\"current_phase\"]}')
          for phase, info in data['phases'].items():
          print(f'{phase}: {info[\"completion\"]}% - {info[\"status\"]}')
          "
          fi
