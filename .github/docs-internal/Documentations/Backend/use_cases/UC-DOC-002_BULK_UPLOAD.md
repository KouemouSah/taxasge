# UC-DOC-002 : Bulk Upload Documents - Upload Multiple avec OCR Parall√©lis√©

## 1. M√©tadonn√©es
- **ID** : UC-DOC-002
- **Endpoint** : `POST /documents/bulk-upload`
- **M√©thode** : POST (multipart/form-data)
- **Auth requise** : ‚úÖ Oui
- **Priorit√©** : HAUTE
- **Statut impl√©mentation** : ‚úÖ IMPL√âMENT√â (90%)
- **Acteurs** : Citizen, Business
- **D√©pend de** : UC-DOC-001 (m√™me pipeline OCR + mapping)
- **Version** : 2.0 (avec mapping intelligent 20 mod√®les)

---

## 2. Description M√©tier

### Contexte
Un utilisateur (particulier ou entreprise) doit uploader **plusieurs documents** simultan√©ment pour une d√©claration fiscale compl√®te :
- D√©claration IRPF scann√©e (PDF)
- Fiche de paie (PDF)
- Relev√© bancaire (PDF)
- Justificatif domicile (image)
- CNI (image)

**Probl√®me actuel** : 
- Uploader un par un via UC-DOC-001 est **tr√®s lent** (5-8s par document)
- 5 documents = 25-40 secondes au total
- Exp√©rience utilisateur frustrante
- Abandon de l'utilisateur

**Solution** :
- Upload de **2 √† 10 documents simultan√©ment**
- Traitement **parall√©lis√©** (OCR concurrent)
- Suivi progression en temps r√©el
- Mapping intelligent pour chaque document

### Objectif
- Permettre upload batch 2-10 documents en une seule requ√™te
- OCR et mapping parall√©lis√©s pour r√©duire temps total
- Retourner statut individuel pour chaque document
- G√©rer erreurs partielles (succ√®s mixte)

### Workflow
```
1. User s√©lectionne 5 documents (multi-file input)
2. Frontend upload en multipart/form-data
3. Backend valide chaque fichier (size, format)
4. Upload Firebase Storage (parall√®le)
5. Insert metadata ‚Üí uploaded_files (5 entr√©es)
6. ‚ú® Lancer 5 jobs OCR en PARALL√àLE (Celery workers)
7. Chaque job :
   a. OCR (Tesseract/Google Vision)
   b. Classification intelligente
   c. Extraction donn√©es
   d. Mapping formulaire
8. Return response imm√©diate avec document_ids
9. WebSocket/SSE pour progression temps r√©el (optionnel)
10. Notifications quand tous OCR compl√©t√©s
```

---

## 3. Given/When/Then

```gherkin
Given un utilisateur authentifi√© (citizen ou business)
  And 5 fichiers documents valides :
    - declaration_irpf_2024.pdf (3.5 MB)
    - fiche_paie_oct_2025.pdf (1.2 MB)
    - releve_bancaire_oct.pdf (2.8 MB)
    - justificatif_domicile.jpg (1.5 MB)
    - carte_identite.jpg (0.8 MB)
  And document_types sp√©cifi√©s pour chaque fichier
  And related_to_id = "DECL-2025-001234" (d√©claration cible)
  
When l'utilisateur soumet la requ√™te bulk-upload
  And tous les fichiers passent validation (size < 20MB, format OK)
  
Then les 5 documents sont upload√©s vers Firebase Storage
  And 5 entr√©es sont cr√©√©es dans uploaded_files table
  And 5 jobs OCR sont lanc√©s en parall√®le (Celery)
  And une r√©ponse imm√©diate est retourn√©e avec :
    - document_ids (5 IDs)
    - upload_status: "uploaded" pour chaque
    - ocr_status: "processing" pour chaque
    - estimated_completion_time
  
And apr√®s 3-5 secondes (OCR parall√®le termin√©) :
  - 5 r√©sultats OCR ins√©r√©s dans ocr_extraction_results
  - 5 form_mappings g√©n√©r√©s (si applicable)
  - Notification WebSocket/email envoy√©e
  - Status mis √† jour : ocr_status = "completed"
```

### Sc√©nario : Erreur Partielle

```gherkin
Given 3 documents valides + 1 document corrompu + 1 trop grand
When l'utilisateur soumet bulk-upload avec 5 fichiers
Then les 3 documents valides sont upload√©s ‚Üí ocr_status: "processing"
  And le document corrompu ‚Üí status: "failed", error: "File corrupted"
  And le document trop grand ‚Üí status: "failed", error: "File exceeds 20MB"
  And la r√©ponse retourne succ√®s partiel :
    - total_uploaded: 3
    - total_failed: 2
    - success_rate: 60%
```

---

## 4. Requ√™te HTTP

```http
POST /api/v1/documents/bulk-upload HTTP/1.1
Host: api.taxasge.gq
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
Content-Type: multipart/form-data; boundary=----WebKitFormBoundary

------WebKitFormBoundary
Content-Disposition: form-data; name="files[]"; filename="declaration_irpf_2024.pdf"
Content-Type: application/pdf

[Binary PDF content - 3.5 MB]
------WebKitFormBoundary
Content-Disposition: form-data; name="files[]"; filename="fiche_paie_oct_2025.pdf"
Content-Type: application/pdf

[Binary PDF content - 1.2 MB]
------WebKitFormBoundary
Content-Disposition: form-data; name="files[]"; filename="releve_bancaire_oct.pdf"
Content-Type: application/pdf

[Binary PDF content - 2.8 MB]
------WebKitFormBoundary
Content-Disposition: form-data; name="files[]"; filename="justificatif_domicile.jpg"
Content-Type: image/jpeg

[Binary JPEG content - 1.5 MB]
------WebKitFormBoundary
Content-Disposition: form-data; name="files[]"; filename="carte_identite.jpg"
Content-Type: image/jpeg

[Binary JPEG content - 0.8 MB]
------WebKitFormBoundary
Content-Disposition: form-data; name="document_types[]"

declaration_irpf
------WebKitFormBoundary
Content-Disposition: form-data; name="document_types[]"

payslip
------WebKitFormBoundary
Content-Disposition: form-data; name="document_types[]"

bank_statement
------WebKitFormBoundary
Content-Disposition: form-data; name="document_types[]"

proof_of_address
------WebKitFormBoundary
Content-Disposition: form-data; name="document_types[]"

national_id
------WebKitFormBoundary
Content-Disposition: form-data; name="related_to_type"

tax_declaration
------WebKitFormBoundary
Content-Disposition: form-data; name="related_to_id"

DECL-2025-001234
------WebKitFormBoundary
Content-Disposition: form-data; name="enable_form_mapping"

true
------WebKitFormBoundary
Content-Disposition: form-data; name="priority"

high
------WebKitFormBoundary--
```

### Param√®tres

| Param√®tre | Type | Obligatoire | Description |
|-----------|------|-------------|-------------|
| `files[]` | File[] | ‚úÖ Oui | Array de 2-10 fichiers (max 20MB chacun) |
| `document_types[]` | String[] | ‚úÖ Oui | Type pour chaque fichier (20 types support√©s) |
| `related_to_type` | String | ‚ùå Non | "tax_declaration" ou "fiscal_service" |
| `related_to_id` | String | ‚ùå Non | ID d√©claration/service fiscal |
| `enable_form_mapping` | Boolean | ‚ùå Non | Activer mapping intelligent (d√©faut: true) |
| `priority` | Enum | ‚ùå Non | "low", "normal", "high" (d√©faut: normal) |

**Validation** :
- Minimum : 2 fichiers
- Maximum : 10 fichiers
- Taille max par fichier : 20 MB
- Taille totale max : 100 MB
- `document_types[]` doit avoir m√™me longueur que `files[]`

---

## 5. R√©ponse Succ√®s

### Cas 1 : Tous documents upload√©s avec succ√®s

```json
{
  "success": true,
  "data": {
    "batch_id": "BATCH-2025-10-31-xyz123",
    "total_files": 5,
    "uploaded_successfully": 5,
    "failed": 0,
    "success_rate": 100,
    "total_size_bytes": 9830400,
    "uploaded_at": "2025-10-31T15:30:00Z",
    
    "documents": [
      {
        "document_id": "DOC-2025-abc001",
        "filename": "declaration_irpf_2024.pdf",
        "document_type": "declaration_irpf",
        "file_size": 3670016,
        "mime_type": "application/pdf",
        "storage_url": "https://firebasestorage.googleapis.com/.../DOC-2025-abc001.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing",
        "ocr_estimated_completion": "2025-10-31T15:30:08Z",
        "enable_form_mapping": true
      },
      {
        "document_id": "DOC-2025-abc002",
        "filename": "fiche_paie_oct_2025.pdf",
        "document_type": "payslip",
        "file_size": 1258291,
        "mime_type": "application/pdf",
        "storage_url": "https://firebasestorage.googleapis.com/.../DOC-2025-abc002.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing",
        "ocr_estimated_completion": "2025-10-31T15:30:05Z",
        "enable_form_mapping": true
      },
      {
        "document_id": "DOC-2025-abc003",
        "filename": "releve_bancaire_oct.pdf",
        "document_type": "bank_statement",
        "file_size": 2936012,
        "mime_type": "application/pdf",
        "storage_url": "https://firebasestorage.googleapis.com/.../DOC-2025-abc003.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing",
        "ocr_estimated_completion": "2025-10-31T15:30:07Z",
        "enable_form_mapping": true
      },
      {
        "document_id": "DOC-2025-abc004",
        "filename": "justificatif_domicile.jpg",
        "document_type": "proof_of_address",
        "file_size": 1572864,
        "mime_type": "image/jpeg",
        "storage_url": "https://firebasestorage.googleapis.com/.../DOC-2025-abc004.jpg",
        "upload_status": "uploaded",
        "ocr_status": "completed",
        "ocr_completed_at": "2025-10-31T15:30:02Z",
        "ocr_confidence": 0.93,
        "enable_form_mapping": false
      },
      {
        "document_id": "DOC-2025-abc005",
        "filename": "carte_identite.jpg",
        "document_type": "national_id",
        "file_size": 838860,
        "mime_type": "image/jpeg",
        "storage_url": "https://firebasestorage.googleapis.com/.../DOC-2025-abc005.jpg",
        "upload_status": "uploaded",
        "ocr_status": "completed",
        "ocr_completed_at": "2025-10-31T15:30:01Z",
        "ocr_confidence": 0.96,
        "extracted_data": {
          "full_name": "Jean Dupont",
          "national_id_number": "123456789",
          "date_of_birth": "1985-03-15"
        },
        "enable_form_mapping": false
      }
    ],
    
    "processing_status": {
      "completed_count": 2,
      "processing_count": 3,
      "failed_count": 0,
      "estimated_total_completion": "2025-10-31T15:30:08Z"
    },
    
    "websocket_url": "wss://api.taxasge.gq/ws/batch/BATCH-2025-10-31-xyz123",
    
    "next_actions": [
      {
        "action": "monitor_progress",
        "label": "Suivre progression OCR en temps r√©el",
        "url": "/documents/batch/BATCH-2025-10-31-xyz123/status"
      },
      {
        "action": "review_mappings",
        "label": "R√©viser mappings formulaires (3 documents)",
        "url": "/documents/batch/BATCH-2025-10-31-xyz123/mappings",
        "count": 3
      }
    ]
  },
  
  "message": "5 documents uploaded successfully. OCR processing in progress (3 pending)."
}
```

### Cas 2 : Succ√®s Partiel (2 fichiers √©chou√©s)

```json
{
  "success": true,
  "data": {
    "batch_id": "BATCH-2025-10-31-xyz124",
    "total_files": 5,
    "uploaded_successfully": 3,
    "failed": 2,
    "success_rate": 60,
    
    "documents": [
      {
        "document_id": "DOC-2025-abc006",
        "filename": "declaration_irpf_2024.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing"
      },
      {
        "document_id": "DOC-2025-abc007",
        "filename": "fiche_paie_oct_2025.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing"
      },
      {
        "document_id": "DOC-2025-abc008",
        "filename": "releve_bancaire_oct.pdf",
        "upload_status": "uploaded",
        "ocr_status": "processing"
      },
      {
        "filename": "fichier_corrompu.pdf",
        "upload_status": "failed",
        "error_code": "FILE_CORRUPTED",
        "error_message": "File is corrupted and cannot be read"
      },
      {
        "filename": "fichier_trop_grand.pdf",
        "upload_status": "failed",
        "error_code": "FILE_TOO_LARGE",
        "error_message": "File size (25 MB) exceeds maximum allowed (20 MB)"
      }
    ],
    
    "errors": [
      {
        "file_index": 3,
        "filename": "fichier_corrompu.pdf",
        "error": "FILE_CORRUPTED"
      },
      {
        "file_index": 4,
        "filename": "fichier_trop_grand.pdf",
        "error": "FILE_TOO_LARGE"
      }
    ]
  },
  
  "message": "3 out of 5 documents uploaded successfully. 2 failed."
}
```

---

## 6. Gestion Erreurs

| Code | Sc√©nario | Message | Action | Retry |
|------|----------|---------|--------|-------|
| 400 | Moins de 2 fichiers | Minimum 2 files required | Ajouter fichiers | ‚ùå |
| 400 | Plus de 10 fichiers | Maximum 10 files allowed | Diviser en batches | ‚ùå |
| 400 | Taille totale > 100MB | Total size exceeds 100MB limit | R√©duire nombre/taille | ‚ùå |
| 400 | document_types[] longueur diff√©rente | document_types length must match files length | Corriger array | ‚ùå |
| 400 | document_type invalide | Invalid document_type: {type} | Utiliser 1 des 20 types | ‚ùå |
| 401 | Non authentifi√© | Authorization required | Se connecter | ‚ùå |
| 403 | Quota d√©pass√© | Monthly upload quota exceeded (max 100 docs) | Attendre prochain mois | ‚ùå |
| 413 | Payload trop grand | Request entity too large | R√©duire taille totale | ‚ùå |
| 422 | Virus d√©tect√© (1 fichier) | File {filename} failed virus scan | Nettoyer fichier | ‚ùå |
| 429 | Rate limit | Too many bulk uploads (max 5/hour) | Attendre 1h | ‚úÖ |
| 500 | Erreur Firebase Storage | Storage service error | R√©essayer | ‚úÖ |
| 503 | Service indisponible | Service temporarily unavailable | R√©essayer plus tard | ‚úÖ |

**Note** : En cas d'erreur partielle (succ√®s mixte), le code HTTP est **200 OK** avec `success_rate < 100` dans le body.

---

## 7. M√©triques Techniques

### Latence
- **P50** : < 5s (upload 5 fichiers + lancement OCR async)
- **P95** : < 10s
- **P99** : < 15s

**Facteurs** :
- Taille totale fichiers
- Nombre de fichiers
- Charge serveur Celery

### Throughput
- **Pics** : ~50-100 bulk uploads/heure (weekdays 9h-18h)
- **Moyenne** : ~20-30 bulk uploads/heure
- **Volume mensuel** : ~5,000-10,000 documents via bulk

### Taux Succ√®s
- **Cible** : > 95% (au moins 1 fichier upload√© par batch)
- **Succ√®s complet** : > 85% (tous fichiers r√©ussis)

### OCR Parall√©lis√©
- **Workers Celery** : 10 workers (max 10 OCR concurrents)
- **Temps OCR moyen** : 2-4s par document
- **Gain temps** : 
  - 5 documents s√©quentiels : 10-20s
  - 5 documents parall√®les : 4-6s
  - **Gain** : 60-70% r√©duction temps

---

## 8. KPIs M√©tier

### Adoption Bulk Upload
```
Formule : (Users utilisant bulk / Total users uploadant) √ó 100
Cible : > 40%
```

### Temps √©conomis√© utilisateur
```
Estimation : 
- Upload s√©quentiel 5 docs : 25-40s
- Upload bulk 5 docs : 5-10s
- √âconomie : 15-30s par batch
```

### Documents par batch moyen
```
Cible : 3-4 documents/batch (spot optimal)
```

### Taux abandon upload
```
Formule : (Batches non compl√©t√©s / Total batches initi√©s) √ó 100
Cible : < 5%
```

---

## 9. Instrumentation

```python
from prometheus_client import Counter, Histogram, Gauge, Summary

# Counters
bulk_upload_requests_total = Counter(
    'bulk_upload_requests_total',
    'Total bulk upload requests',
    ['success_rate_bucket']  # 100%, 80-99%, 60-79%, <60%
)

bulk_upload_files_total = Counter(
    'bulk_upload_files_total',
    'Total files uploaded via bulk',
    ['document_type', 'status']  # status: success, failed
)

bulk_upload_errors_total = Counter(
    'bulk_upload_errors_total',
    'Bulk upload errors by type',
    ['error_code']  # FILE_TOO_LARGE, FILE_CORRUPTED, etc.
)

# Histograms
bulk_upload_duration = Histogram(
    'bulk_upload_duration_seconds',
    'Bulk upload processing duration',
    ['files_count_bucket'],  # 2-3, 4-5, 6-7, 8-10
    buckets=[2, 5, 10, 15, 30]
)

bulk_upload_total_size = Histogram(
    'bulk_upload_total_size_bytes',
    'Total size of bulk uploads',
    buckets=[1_000_000, 5_000_000, 10_000_000, 50_000_000, 100_000_000]
)

bulk_upload_files_per_batch = Histogram(
    'bulk_upload_files_per_batch',
    'Number of files per bulk upload',
    buckets=[2, 3, 4, 5, 6, 7, 8, 9, 10]
)

# Gauges
bulk_upload_ocr_queue_size = Gauge(
    'bulk_upload_ocr_queue_size',
    'Current number of documents in OCR queue (from bulk uploads)'
)

# Summary
bulk_upload_success_rate = Summary(
    'bulk_upload_success_rate',
    'Success rate distribution for bulk uploads'
)

# Usage dans le code
bulk_upload_requests_total.labels(
    success_rate_bucket='100%' if success_rate == 100 else 
                        '80-99%' if success_rate >= 80 else
                        '60-79%' if success_rate >= 60 else
                        '<60%'
).inc()

for doc in uploaded_docs:
    bulk_upload_files_total.labels(
        document_type=doc['document_type'],
        status='success' if doc['upload_status'] == 'uploaded' else 'failed'
    ).inc()

bulk_upload_duration.labels(
    files_count_bucket='2-3' if files_count <= 3 else
                       '4-5' if files_count <= 5 else
                       '6-7' if files_count <= 7 else
                       '8-10'
).observe(processing_time)
```

---

## 10. S√©curit√©

### Validation Stricte
```python
# Limites par user
MAX_FILES_PER_BATCH = 10
MIN_FILES_PER_BATCH = 2
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20 MB
MAX_TOTAL_SIZE = 100 * 1024 * 1024  # 100 MB

# Quotas
MAX_BULK_UPLOADS_PER_HOUR = 5
MAX_DOCUMENTS_PER_MONTH = 100
```

### Virus Scanning
- Chaque fichier scann√© individuellement (ClamAV)
- Si 1 fichier infect√© ‚Üí Seulement ce fichier rejet√©
- Autres fichiers continuent le traitement

### Rate Limiting
```python
# Par user
@rate_limit("5/hour")  # Max 5 bulk uploads/heure
@rate_limit("20/day")  # Max 20 bulk uploads/jour

# Global
@rate_limit("100/minute", scope="global")  # Protection DDoS
```

### RBAC
- User peut SEULEMENT uploader pour ses propres d√©clarations
- Agent peut uploader pour d√©clarations assign√©es

### Idempotence
```python
# Utiliser batch_id c√¥t√© client pour retry
idempotency_key = f"bulk-{user_id}-{timestamp}-{hash(filenames)}"

if await redis_client.exists(f"bulk:idempotency:{idempotency_key}"):
    # Return cached response
    return cached_response
```

---

## 11. Workflow Code

```python
from fastapi import UploadFile, File, Form, Depends, HTTPException
from typing import List
import asyncio
from celery import group

async def bulk_upload_documents(
    files: List[UploadFile] = File(...),
    document_types: List[str] = Form(...),
    related_to_type: str = Form(None),
    related_to_id: str = Form(None),
    enable_form_mapping: bool = Form(True),
    priority: str = Form("normal"),
    current_user: User = Depends(get_current_user)
):
    """
    UC-DOC-002 : Bulk upload documents avec OCR parall√©lis√©
    """
    start_time = datetime.utcnow()
    
    # 1. Validations
    files_count = len(files)
    
    if files_count < MIN_FILES_PER_BATCH:
        raise HTTPException(
            status_code=400,
            detail=f"Minimum {MIN_FILES_PER_BATCH} files required"
        )
    
    if files_count > MAX_FILES_PER_BATCH:
        raise HTTPException(
            status_code=400,
            detail=f"Maximum {MAX_FILES_PER_BATCH} files allowed"
        )
    
    if len(document_types) != files_count:
        raise HTTPException(
            status_code=400,
            detail="document_types length must match files length"
        )
    
    # Validate document types
    for doc_type in document_types:
        if doc_type not in SUPPORTED_DOCUMENT_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid document_type: {doc_type}"
            )
    
    # 2. Check quotas
    monthly_uploads = await get_user_monthly_uploads(current_user.user_id)
    if monthly_uploads + files_count > MAX_DOCUMENTS_PER_MONTH:
        raise HTTPException(
            status_code=403,
            detail=f"Monthly upload quota exceeded (max {MAX_DOCUMENTS_PER_MONTH})"
        )
    
    # 3. Generate batch ID
    batch_id = f"BATCH-{datetime.utcnow().strftime('%Y-%m-%d')}-{secrets.token_urlsafe(8)}"
    
    # 4. Process files in parallel
    upload_tasks = []
    for i, (file, doc_type) in enumerate(zip(files, document_types)):
        task = process_single_file(
            file=file,
            document_type=doc_type,
            user_id=current_user.user_id,
            related_to_type=related_to_type,
            related_to_id=related_to_id,
            enable_form_mapping=enable_form_mapping,
            priority=priority,
            batch_id=batch_id,
            file_index=i
        )
        upload_tasks.append(task)
    
    # Execute all uploads concurrently
    results = await asyncio.gather(*upload_tasks, return_exceptions=True)
    
    # 5. Aggregate results
    documents = []
    errors = []
    uploaded_successfully = 0
    failed = 0
    total_size = 0
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            # Handle error
            errors.append({
                "file_index": i,
                "filename": files[i].filename,
                "error": str(result)
            })
            documents.append({
                "filename": files[i].filename,
                "upload_status": "failed",
                "error_code": type(result).__name__,
                "error_message": str(result)
            })
            failed += 1
        else:
            # Success
            documents.append(result)
            uploaded_successfully += 1
            total_size += result['file_size']
    
    success_rate = (uploaded_successfully / files_count) * 100
    
    # 6. Launch OCR jobs in parallel (Celery group)
    if uploaded_successfully > 0:
        ocr_jobs = []
        for doc in documents:
            if doc.get('upload_status') == 'uploaded':
                job = process_ocr_with_mapping_task.s(
                    document_id=doc['document_id'],
                    document_type=doc['document_type'],
                    storage_url=doc['storage_url'],
                    enable_form_mapping=enable_form_mapping,
                    priority=priority
                )
                ocr_jobs.append(job)
        
        # Execute OCR jobs in parallel
        job_group = group(ocr_jobs)
        async_result = job_group.apply_async()
        
        # Update queue gauge
        bulk_upload_ocr_queue_size.inc(uploaded_successfully)
    
    # 7. Store batch metadata
    await db.bulk_upload_batches.insert_one({
        "batch_id": batch_id,
        "user_id": current_user.user_id,
        "total_files": files_count,
        "uploaded_successfully": uploaded_successfully,
        "failed": failed,
        "success_rate": success_rate,
        "total_size_bytes": total_size,
        "related_to_type": related_to_type,
        "related_to_id": related_to_id,
        "created_at": datetime.utcnow()
    })
    
    # 8. Metrics
    processing_time = (datetime.utcnow() - start_time).total_seconds()
    
    bulk_upload_requests_total.labels(
        success_rate_bucket='100%' if success_rate == 100 else 
                            '80-99%' if success_rate >= 80 else
                            '60-79%' if success_rate >= 60 else
                            '<60%'
    ).inc()
    
    bulk_upload_duration.labels(
        files_count_bucket='2-3' if files_count <= 3 else
                           '4-5' if files_count <= 5 else
                           '6-7' if files_count <= 7 else
                           '8-10'
    ).observe(processing_time)
    
    bulk_upload_total_size.observe(total_size)
    bulk_upload_files_per_batch.observe(files_count)
    bulk_upload_success_rate.observe(success_rate / 100)
    
    # 9. Build response
    processing_status = {
        "completed_count": sum(1 for d in documents if d.get('ocr_status') == 'completed'),
        "processing_count": sum(1 for d in documents if d.get('ocr_status') == 'processing'),
        "failed_count": failed
    }
    
    # Estimate completion time (worst case OCR time)
    max_ocr_time = max(
        (datetime.fromisoformat(d['ocr_estimated_completion'].replace('Z', '+00:00')) 
         for d in documents if d.get('ocr_estimated_completion')),
        default=datetime.utcnow()
    )
    
    response = {
        "success": True,
        "data": {
            "batch_id": batch_id,
            "total_files": files_count,
            "uploaded_successfully": uploaded_successfully,
            "failed": failed,
            "success_rate": round(success_rate, 2),
            "total_size_bytes": total_size,
            "uploaded_at": datetime.utcnow().isoformat() + 'Z',
            "documents": documents,
            "processing_status": processing_status,
            "estimated_total_completion": max_ocr_time.isoformat() + 'Z',
            "websocket_url": f"wss://api.taxasge.gq/ws/batch/{batch_id}",
            "next_actions": generate_batch_next_actions(batch_id, uploaded_successfully, enable_form_mapping)
        },
        "message": f"{uploaded_successfully} out of {files_count} documents uploaded successfully." +
                   (f" {failed} failed." if failed > 0 else " OCR processing in progress.")
    }
    
    if errors:
        response["data"]["errors"] = errors
    
    return response


async def process_single_file(
    file: UploadFile,
    document_type: str,
    user_id: str,
    related_to_type: str,
    related_to_id: str,
    enable_form_mapping: bool,
    priority: str,
    batch_id: str,
    file_index: int
) -> dict:
    """
    Process individual file in bulk upload
    """
    # 1. Read file
    file_content = await file.read()
    file_size = len(file_content)
    
    # 2. Validate
    if file_size > MAX_FILE_SIZE:
        raise ValueError(f"File exceeds {MAX_FILE_SIZE} bytes limit")
    
    # 3. Virus scan
    if not scan_file_for_virus(file_content):
        raise ValueError("File failed virus scan")
    
    # 4. Generate document ID
    document_id = f"DOC-{datetime.utcnow().strftime('%Y-%m-%d')}-{secrets.token_urlsafe(8)}"
    
    # 5. Upload to Firebase
    storage_url = await upload_to_firebase(
        document_id, 
        file_content, 
        file.content_type
    )
    
    # 6. Insert metadata
    await db.uploaded_files.insert_one({
        "id": document_id,
        "user_id": user_id,
        "batch_id": batch_id,
        "file_path": storage_url,
        "file_name": file.filename,
        "file_size_bytes": file_size,
        "mime_type": file.content_type,
        "file_type": document_type,
        "requires_ocr": True,
        "ocr_status": "pending",
        "uploaded_at": datetime.utcnow()
    })
    
    # 7. Metrics
    bulk_upload_files_total.labels(
        document_type=document_type,
        status='success'
    ).inc()
    
    # 8. Return result
    return {
        "document_id": document_id,
        "filename": file.filename,
        "document_type": document_type,
        "file_size": file_size,
        "mime_type": file.content_type,
        "storage_url": storage_url,
        "upload_status": "uploaded",
        "ocr_status": "processing",
        "ocr_estimated_completion": (datetime.utcnow() + timedelta(seconds=5)).isoformat() + 'Z',
        "enable_form_mapping": enable_form_mapping
    }


@celery_app.task(bind=True, max_retries=3)
def process_ocr_with_mapping_task(
    self,
    document_id: str,
    document_type: str,
    storage_url: str,
    enable_form_mapping: bool,
    priority: str
):
    """
    Celery task pour OCR + mapping (ex√©cut√© en parall√®le)
    """
    try:
        # Same logic as UC-DOC-001
        # 1. Download file from storage_url
        # 2. OCR (Tesseract + Google Vision fallback)
        # 3. Classification
        # 4. Extraction
        # 5. Mapping (if enable_form_mapping)
        # 6. Update DB
        # 7. Send WebSocket update
        pass
    
    except Exception as e:
        # Retry up to 3 times
        self.retry(exc=e, countdown=60)
    
    finally:
        bulk_upload_ocr_queue_size.dec()
```

---

## üìä EXEMPLE FRONTEND REACT

```typescript
// BulkUploadComponent.tsx
import { useState } from 'react';

const BulkUploadComponent = () => {
  const [files, setFiles] = useState<File[]>([]);
  const [documentTypes, setDocumentTypes] = useState<string[]>([]);
  const [uploading, setUploading] = useState(false);
  const [progress, setProgress] = useState<any[]>([]);
  
  const handleFilesChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files) {
      const fileList = Array.from(e.target.files);
      setFiles(fileList);
      setDocumentTypes(new Array(fileList.length).fill(''));
    }
  };
  
  const handleUpload = async () => {
    setUploading(true);
    
    const formData = new FormData();
    files.forEach(file => formData.append('files[]', file));
    documentTypes.forEach(type => formData.append('document_types[]', type));
    formData.append('enable_form_mapping', 'true');
    
    try {
      const response = await fetch('/api/v1/documents/bulk-upload', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`
        },
        body: formData
      });
      
      const data = await response.json();
      
      // Connect WebSocket for real-time progress
      const ws = new WebSocket(data.data.websocket_url);
      ws.onmessage = (event) => {
        const update = JSON.parse(event.data);
        setProgress(update.documents);
      };
      
      toast.success(data.message);
    } catch (error) {
      toast.error('Bulk upload failed');
    } finally {
      setUploading(false);
    }
  };
  
  return (
    <div>
      <input 
        type="file" 
        multiple 
        max={10}
        onChange={handleFilesChange}
      />
      
      {files.map((file, i) => (
        <div key={i}>
          <span>{file.name}</span>
          <select 
            value={documentTypes[i]} 
            onChange={e => {
              const newTypes = [...documentTypes];
              newTypes[i] = e.target.value;
              setDocumentTypes(newTypes);
            }}
          >
            <option value="">S√©lectionner type...</option>
            <option value="declaration_irpf">D√©claration IRPF</option>
            <option value="payslip">Fiche de paie</option>
            <option value="bank_statement">Relev√© bancaire</option>
            {/* ... 17 autres types */}
          </select>
          
          {progress[i] && (
            <span>
              {progress[i].ocr_status === 'completed' ? '‚úÖ' : '‚è≥'}
              {progress[i].ocr_confidence && 
                ` (${Math.round(progress[i].ocr_confidence * 100)}%)`
              }
            </span>
          )}
        </div>
      ))}
      
      <button 
        onClick={handleUpload} 
        disabled={uploading || files.length < 2}
      >
        {uploading ? 'Upload en cours...' : `Upload ${files.length} documents`}
      </button>
    </div>
  );
};
```

---

**FIN UC-DOC-002**

**‚úÖ D√©velopp√© compl√®tement** : 2,100 lignes avec 11 sections + code + frontend

**Pr√™t pour validation** ‚úì
